import os
import numpy as np
import tempfile, zipfile
import torch
import torch.nn as nn
import torch.nn.functional as F
try:
    import torchvision
except:
    pass

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

        self.backbone_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=3, kernel_size=(7,7), out_channels=64, padding=(3,3), padding_mode='zeros', stride=(2,2))
        self.backbone_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_relu1 = nn.ReLU()
        self.backbone_layer1_0_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=64, kernel_size=(3,3), out_channels=64, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer1_0_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_layer1_0_relu = nn.ReLU()
        self.backbone_layer1_0_conv2 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=64, kernel_size=(3,3), out_channels=64, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer1_0_norm2 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_0 = nn.ReLU()
        self.pnnx_unique_1 = nn.ReLU()
        self.backbone_layer1_1_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=64, kernel_size=(3,3), out_channels=64, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer1_1_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_layer1_1_relu = nn.ReLU()
        self.backbone_layer1_1_conv2 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=64, kernel_size=(3,3), out_channels=64, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer1_1_norm2 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_2 = nn.ReLU()
        self.pnnx_unique_3 = nn.ReLU()
        self.backbone_layer2_0_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=64, kernel_size=(3,3), out_channels=96, padding=(1,1), padding_mode='zeros', stride=(2,2))
        self.backbone_layer2_0_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_layer2_0_relu = nn.ReLU()
        self.backbone_layer2_0_conv2 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=96, kernel_size=(3,3), out_channels=96, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer2_0_norm2 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_4 = nn.ReLU()
        self.backbone_layer2_0_downsample_0 = nn.Conv2d(bias=True, dilation=(1,1), groups=1, in_channels=64, kernel_size=(1,1), out_channels=96, padding=(0,0), padding_mode='zeros', stride=(2,2))
        self.backbone_layer2_0_downsample_1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_5 = nn.ReLU()
        self.backbone_layer2_1_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=96, kernel_size=(3,3), out_channels=96, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer2_1_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_layer2_1_relu = nn.ReLU()
        self.backbone_layer2_1_conv2 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=96, kernel_size=(3,3), out_channels=96, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer2_1_norm2 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_6 = nn.ReLU()
        self.pnnx_unique_7 = nn.ReLU()
        self.backbone_layer3_0_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=96, kernel_size=(3,3), out_channels=128, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer3_0_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_layer3_0_relu = nn.ReLU()
        self.backbone_layer3_0_conv2 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=128, kernel_size=(3,3), out_channels=128, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer3_0_norm2 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_8 = nn.ReLU()
        self.backbone_layer3_0_downsample_0 = nn.Conv2d(bias=True, dilation=(1,1), groups=1, in_channels=96, kernel_size=(1,1), out_channels=128, padding=(0,0), padding_mode='zeros', stride=(1,1))
        self.backbone_layer3_0_downsample_1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_9 = nn.ReLU()
        self.backbone_layer3_1_conv1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=128, kernel_size=(3,3), out_channels=128, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer3_1_norm1 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.backbone_layer3_1_relu = nn.ReLU()
        self.backbone_layer3_1_conv2 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=128, kernel_size=(3,3), out_channels=128, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.backbone_layer3_1_norm2 = nn.InstanceNorm2d(affine=False, eps=0.000010, track_running_stats=False)
        self.pnnx_unique_10 = nn.ReLU()
        self.pnnx_unique_11 = nn.ReLU()
        self.backbone_conv2 = nn.Conv2d(bias=True, dilation=(1,1), groups=1, in_channels=128, kernel_size=(1,1), out_channels=128, padding=(0,0), padding_mode='zeros', stride=(1,1))
        self.conv2d_0 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=128, kernel_size=(3,3), out_channels=128, padding=(1,1), padding_mode='zeros', stride=(2,2))
        self.conv2d_1 = nn.Conv2d(bias=False, dilation=(1,1), groups=1, in_channels=128, kernel_size=(3,3), out_channels=128, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.transformer_layers_0_self_attn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_self_attn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_self_attn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_self_attn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_self_attn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_0_cross_attn_ffn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_cross_attn_ffn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_cross_attn_ffn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_cross_attn_ffn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_0_cross_attn_ffn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_0_cross_attn_ffn_mlp_0 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.transformer_layers_0_cross_attn_ffn_mlp_1 = nn.GELU()
        self.transformer_layers_0_cross_attn_ffn_mlp_2 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.transformer_layers_0_cross_attn_ffn_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_1_self_attn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_self_attn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_self_attn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_self_attn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_self_attn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_1_cross_attn_ffn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_cross_attn_ffn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_cross_attn_ffn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_cross_attn_ffn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_1_cross_attn_ffn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_1_cross_attn_ffn_mlp_0 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.transformer_layers_1_cross_attn_ffn_mlp_1 = nn.GELU()
        self.transformer_layers_1_cross_attn_ffn_mlp_2 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.transformer_layers_1_cross_attn_ffn_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_2_self_attn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_self_attn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_self_attn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_self_attn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_self_attn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_2_cross_attn_ffn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_cross_attn_ffn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_cross_attn_ffn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_cross_attn_ffn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_2_cross_attn_ffn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_2_cross_attn_ffn_mlp_0 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.transformer_layers_2_cross_attn_ffn_mlp_1 = nn.GELU()
        self.transformer_layers_2_cross_attn_ffn_mlp_2 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.transformer_layers_2_cross_attn_ffn_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_3_self_attn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_self_attn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_self_attn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_self_attn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_self_attn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_3_cross_attn_ffn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_cross_attn_ffn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_cross_attn_ffn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_cross_attn_ffn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_3_cross_attn_ffn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_3_cross_attn_ffn_mlp_0 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.transformer_layers_3_cross_attn_ffn_mlp_1 = nn.GELU()
        self.transformer_layers_3_cross_attn_ffn_mlp_2 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.transformer_layers_3_cross_attn_ffn_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_4_self_attn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_self_attn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_self_attn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_self_attn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_self_attn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_4_cross_attn_ffn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_cross_attn_ffn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_cross_attn_ffn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_cross_attn_ffn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_4_cross_attn_ffn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_4_cross_attn_ffn_mlp_0 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.transformer_layers_4_cross_attn_ffn_mlp_1 = nn.GELU()
        self.transformer_layers_4_cross_attn_ffn_mlp_2 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.transformer_layers_4_cross_attn_ffn_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_5_self_attn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_self_attn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_self_attn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_self_attn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_self_attn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_5_cross_attn_ffn_q_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_cross_attn_ffn_k_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_cross_attn_ffn_v_proj = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_cross_attn_ffn_merge = nn.Linear(bias=False, in_features=128, out_features=128)
        self.transformer_layers_5_cross_attn_ffn_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.transformer_layers_5_cross_attn_ffn_mlp_0 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.transformer_layers_5_cross_attn_ffn_mlp_1 = nn.GELU()
        self.transformer_layers_5_cross_attn_ffn_mlp_2 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.transformer_layers_5_cross_attn_ffn_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.feature_flow_attn_q_proj = nn.Linear(bias=True, in_features=128, out_features=128)
        self.feature_flow_attn_k_proj = nn.Linear(bias=True, in_features=128, out_features=128)
        self.pnnx_unique_12 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_13 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_14 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_15 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_16 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_17 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_18 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_19 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_20 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_21 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_22 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.pnnx_unique_23 = nn.GELU()
        self.pnnx_unique_24 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.pnnx_unique_25 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_26 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_27 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_28 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_29 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_30 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_31 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_32 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_33 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_34 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_35 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_36 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.pnnx_unique_37 = nn.GELU()
        self.pnnx_unique_38 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.pnnx_unique_39 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_40 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_41 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_42 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_43 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_44 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_45 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_46 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_47 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_48 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_49 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_50 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.pnnx_unique_51 = nn.GELU()
        self.pnnx_unique_52 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.pnnx_unique_53 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_54 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_55 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_56 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_57 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_58 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_59 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_60 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_61 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_62 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_63 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_64 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.pnnx_unique_65 = nn.GELU()
        self.pnnx_unique_66 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.pnnx_unique_67 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_68 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_69 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_70 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_71 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_72 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_73 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_74 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_75 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_76 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_77 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_78 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.pnnx_unique_79 = nn.GELU()
        self.pnnx_unique_80 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.pnnx_unique_81 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_82 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_83 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_84 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_85 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_86 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_87 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_88 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_89 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_90 = nn.Linear(bias=False, in_features=128, out_features=128)
        self.pnnx_unique_91 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_92 = nn.Linear(bias=False, in_features=256, out_features=1024)
        self.pnnx_unique_93 = nn.GELU()
        self.pnnx_unique_94 = nn.Linear(bias=False, in_features=1024, out_features=128)
        self.pnnx_unique_95 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(128,))
        self.pnnx_unique_96 = nn.Linear(bias=True, in_features=128, out_features=128)
        self.pnnx_unique_97 = nn.Linear(bias=True, in_features=128, out_features=128)
        self.upsampler_0 = nn.Conv2d(bias=True, dilation=(1,1), groups=1, in_channels=130, kernel_size=(3,3), out_channels=256, padding=(1,1), padding_mode='zeros', stride=(1,1))
        self.upsampler_1 = nn.ReLU()
        self.upsampler_2 = nn.Conv2d(bias=True, dilation=(1,1), groups=1, in_channels=256, kernel_size=(1,1), out_channels=144, padding=(0,0), padding_mode='zeros', stride=(1,1))

        archive = zipfile.ZipFile('flownet_544.pnnx.bin', 'r')
        self.backbone_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.conv1.weight', (64,3,7,7), 'float32')
        self.backbone_layer1_0_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer1.0.conv1.weight', (64,64,3,3), 'float32')
        self.backbone_layer1_0_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer1.0.conv2.weight', (64,64,3,3), 'float32')
        self.backbone_layer1_1_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer1.1.conv1.weight', (64,64,3,3), 'float32')
        self.backbone_layer1_1_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer1.1.conv2.weight', (64,64,3,3), 'float32')
        self.backbone_layer2_0_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer2.0.conv1.weight', (96,64,3,3), 'float32')
        self.backbone_layer2_0_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer2.0.conv2.weight', (96,96,3,3), 'float32')
        self.backbone_layer2_0_downsample_0.bias = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer2.0.downsample.0.bias', (96), 'float32')
        self.backbone_layer2_0_downsample_0.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer2.0.downsample.0.weight', (96,64,1,1), 'float32')
        self.backbone_layer2_1_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer2.1.conv1.weight', (96,96,3,3), 'float32')
        self.backbone_layer2_1_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer2.1.conv2.weight', (96,96,3,3), 'float32')
        self.backbone_layer3_0_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer3.0.conv1.weight', (128,96,3,3), 'float32')
        self.backbone_layer3_0_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer3.0.conv2.weight', (128,128,3,3), 'float32')
        self.backbone_layer3_0_downsample_0.bias = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer3.0.downsample.0.bias', (128), 'float32')
        self.backbone_layer3_0_downsample_0.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer3.0.downsample.0.weight', (128,96,1,1), 'float32')
        self.backbone_layer3_1_conv1.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer3.1.conv1.weight', (128,128,3,3), 'float32')
        self.backbone_layer3_1_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.layer3.1.conv2.weight', (128,128,3,3), 'float32')
        self.backbone_conv2.bias = self.load_pnnx_bin_as_parameter(archive, 'backbone.conv2.bias', (128), 'float32')
        self.backbone_conv2.weight = self.load_pnnx_bin_as_parameter(archive, 'backbone.conv2.weight', (128,128,1,1), 'float32')
        self.conv2d_0.weight = self.load_pnnx_bin_as_parameter(archive, 'conv2d_0.weight', (128,128,3,3), 'float32')
        self.conv2d_1.weight = self.load_pnnx_bin_as_parameter(archive, 'conv2d_1.weight', (128,128,3,3), 'float32')
        self.transformer_layers_0_self_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.self_attn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_0_self_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.self_attn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_0_self_attn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.self_attn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_0_self_attn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.self_attn.merge.weight', (128,128), 'float32')
        self.transformer_layers_0_self_attn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.self_attn.norm1.bias', (128), 'float32')
        self.transformer_layers_0_self_attn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.self_attn.norm1.weight', (128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.merge.weight', (128,128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.norm1.bias', (128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.norm1.weight', (128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_mlp_0.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.mlp.0.weight', (1024,256), 'float32')
        self.transformer_layers_0_cross_attn_ffn_mlp_2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.mlp.2.weight', (128,1024), 'float32')
        self.transformer_layers_0_cross_attn_ffn_norm2.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.norm2.bias', (128), 'float32')
        self.transformer_layers_0_cross_attn_ffn_norm2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.0.cross_attn_ffn.norm2.weight', (128), 'float32')
        self.transformer_layers_1_self_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.self_attn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_1_self_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.self_attn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_1_self_attn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.self_attn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_1_self_attn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.self_attn.merge.weight', (128,128), 'float32')
        self.transformer_layers_1_self_attn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.self_attn.norm1.bias', (128), 'float32')
        self.transformer_layers_1_self_attn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.self_attn.norm1.weight', (128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.merge.weight', (128,128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.norm1.bias', (128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.norm1.weight', (128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_mlp_0.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.mlp.0.weight', (1024,256), 'float32')
        self.transformer_layers_1_cross_attn_ffn_mlp_2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.mlp.2.weight', (128,1024), 'float32')
        self.transformer_layers_1_cross_attn_ffn_norm2.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.norm2.bias', (128), 'float32')
        self.transformer_layers_1_cross_attn_ffn_norm2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.1.cross_attn_ffn.norm2.weight', (128), 'float32')
        self.transformer_layers_2_self_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.self_attn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_2_self_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.self_attn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_2_self_attn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.self_attn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_2_self_attn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.self_attn.merge.weight', (128,128), 'float32')
        self.transformer_layers_2_self_attn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.self_attn.norm1.bias', (128), 'float32')
        self.transformer_layers_2_self_attn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.self_attn.norm1.weight', (128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.merge.weight', (128,128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.norm1.bias', (128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.norm1.weight', (128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_mlp_0.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.mlp.0.weight', (1024,256), 'float32')
        self.transformer_layers_2_cross_attn_ffn_mlp_2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.mlp.2.weight', (128,1024), 'float32')
        self.transformer_layers_2_cross_attn_ffn_norm2.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.norm2.bias', (128), 'float32')
        self.transformer_layers_2_cross_attn_ffn_norm2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.2.cross_attn_ffn.norm2.weight', (128), 'float32')
        self.transformer_layers_3_self_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.self_attn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_3_self_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.self_attn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_3_self_attn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.self_attn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_3_self_attn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.self_attn.merge.weight', (128,128), 'float32')
        self.transformer_layers_3_self_attn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.self_attn.norm1.bias', (128), 'float32')
        self.transformer_layers_3_self_attn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.self_attn.norm1.weight', (128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.merge.weight', (128,128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.norm1.bias', (128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.norm1.weight', (128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_mlp_0.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.mlp.0.weight', (1024,256), 'float32')
        self.transformer_layers_3_cross_attn_ffn_mlp_2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.mlp.2.weight', (128,1024), 'float32')
        self.transformer_layers_3_cross_attn_ffn_norm2.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.norm2.bias', (128), 'float32')
        self.transformer_layers_3_cross_attn_ffn_norm2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.3.cross_attn_ffn.norm2.weight', (128), 'float32')
        self.transformer_layers_4_self_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.self_attn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_4_self_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.self_attn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_4_self_attn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.self_attn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_4_self_attn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.self_attn.merge.weight', (128,128), 'float32')
        self.transformer_layers_4_self_attn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.self_attn.norm1.bias', (128), 'float32')
        self.transformer_layers_4_self_attn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.self_attn.norm1.weight', (128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.merge.weight', (128,128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.norm1.bias', (128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.norm1.weight', (128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_mlp_0.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.mlp.0.weight', (1024,256), 'float32')
        self.transformer_layers_4_cross_attn_ffn_mlp_2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.mlp.2.weight', (128,1024), 'float32')
        self.transformer_layers_4_cross_attn_ffn_norm2.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.norm2.bias', (128), 'float32')
        self.transformer_layers_4_cross_attn_ffn_norm2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.4.cross_attn_ffn.norm2.weight', (128), 'float32')
        self.transformer_layers_5_self_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.self_attn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_5_self_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.self_attn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_5_self_attn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.self_attn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_5_self_attn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.self_attn.merge.weight', (128,128), 'float32')
        self.transformer_layers_5_self_attn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.self_attn.norm1.bias', (128), 'float32')
        self.transformer_layers_5_self_attn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.self_attn.norm1.weight', (128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.q_proj.weight', (128,128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.k_proj.weight', (128,128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_v_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.v_proj.weight', (128,128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_merge.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.merge.weight', (128,128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_norm1.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.norm1.bias', (128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_norm1.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.norm1.weight', (128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_mlp_0.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.mlp.0.weight', (1024,256), 'float32')
        self.transformer_layers_5_cross_attn_ffn_mlp_2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.mlp.2.weight', (128,1024), 'float32')
        self.transformer_layers_5_cross_attn_ffn_norm2.bias = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.norm2.bias', (128), 'float32')
        self.transformer_layers_5_cross_attn_ffn_norm2.weight = self.load_pnnx_bin_as_parameter(archive, 'transformer.layers.5.cross_attn_ffn.norm2.weight', (128), 'float32')
        self.feature_flow_attn_q_proj.bias = self.load_pnnx_bin_as_parameter(archive, 'feature_flow_attn.q_proj.bias', (128), 'float32')
        self.feature_flow_attn_q_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'feature_flow_attn.q_proj.weight', (128,128), 'float32')
        self.feature_flow_attn_k_proj.bias = self.load_pnnx_bin_as_parameter(archive, 'feature_flow_attn.k_proj.bias', (128), 'float32')
        self.feature_flow_attn_k_proj.weight = self.load_pnnx_bin_as_parameter(archive, 'feature_flow_attn.k_proj.weight', (128,128), 'float32')
        self.pnnx_unique_12.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_12.weight', (128,128), 'float32')
        self.pnnx_unique_13.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_13.weight', (128,128), 'float32')
        self.pnnx_unique_14.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_14.weight', (128,128), 'float32')
        self.pnnx_unique_15.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_15.weight', (128,128), 'float32')
        self.pnnx_unique_16.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_16.bias', (128), 'float32')
        self.pnnx_unique_16.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_16.weight', (128), 'float32')
        self.pnnx_unique_17.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_17.weight', (128,128), 'float32')
        self.pnnx_unique_18.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_18.weight', (128,128), 'float32')
        self.pnnx_unique_19.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_19.weight', (128,128), 'float32')
        self.pnnx_unique_20.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_20.weight', (128,128), 'float32')
        self.pnnx_unique_21.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_21.bias', (128), 'float32')
        self.pnnx_unique_21.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_21.weight', (128), 'float32')
        self.pnnx_unique_22.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_22.weight', (1024,256), 'float32')
        self.pnnx_unique_24.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_24.weight', (128,1024), 'float32')
        self.pnnx_unique_25.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_25.bias', (128), 'float32')
        self.pnnx_unique_25.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_25.weight', (128), 'float32')
        self.pnnx_unique_26.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_26.weight', (128,128), 'float32')
        self.pnnx_unique_27.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_27.weight', (128,128), 'float32')
        self.pnnx_unique_28.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_28.weight', (128,128), 'float32')
        self.pnnx_unique_29.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_29.weight', (128,128), 'float32')
        self.pnnx_unique_30.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_30.bias', (128), 'float32')
        self.pnnx_unique_30.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_30.weight', (128), 'float32')
        self.pnnx_unique_31.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_31.weight', (128,128), 'float32')
        self.pnnx_unique_32.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_32.weight', (128,128), 'float32')
        self.pnnx_unique_33.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_33.weight', (128,128), 'float32')
        self.pnnx_unique_34.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_34.weight', (128,128), 'float32')
        self.pnnx_unique_35.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_35.bias', (128), 'float32')
        self.pnnx_unique_35.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_35.weight', (128), 'float32')
        self.pnnx_unique_36.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_36.weight', (1024,256), 'float32')
        self.pnnx_unique_38.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_38.weight', (128,1024), 'float32')
        self.pnnx_unique_39.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_39.bias', (128), 'float32')
        self.pnnx_unique_39.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_39.weight', (128), 'float32')
        self.pnnx_unique_40.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_40.weight', (128,128), 'float32')
        self.pnnx_unique_41.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_41.weight', (128,128), 'float32')
        self.pnnx_unique_42.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_42.weight', (128,128), 'float32')
        self.pnnx_unique_43.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_43.weight', (128,128), 'float32')
        self.pnnx_unique_44.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_44.bias', (128), 'float32')
        self.pnnx_unique_44.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_44.weight', (128), 'float32')
        self.pnnx_unique_45.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_45.weight', (128,128), 'float32')
        self.pnnx_unique_46.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_46.weight', (128,128), 'float32')
        self.pnnx_unique_47.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_47.weight', (128,128), 'float32')
        self.pnnx_unique_48.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_48.weight', (128,128), 'float32')
        self.pnnx_unique_49.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_49.bias', (128), 'float32')
        self.pnnx_unique_49.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_49.weight', (128), 'float32')
        self.pnnx_unique_50.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_50.weight', (1024,256), 'float32')
        self.pnnx_unique_52.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_52.weight', (128,1024), 'float32')
        self.pnnx_unique_53.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_53.bias', (128), 'float32')
        self.pnnx_unique_53.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_53.weight', (128), 'float32')
        self.pnnx_unique_54.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_54.weight', (128,128), 'float32')
        self.pnnx_unique_55.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_55.weight', (128,128), 'float32')
        self.pnnx_unique_56.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_56.weight', (128,128), 'float32')
        self.pnnx_unique_57.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_57.weight', (128,128), 'float32')
        self.pnnx_unique_58.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_58.bias', (128), 'float32')
        self.pnnx_unique_58.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_58.weight', (128), 'float32')
        self.pnnx_unique_59.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_59.weight', (128,128), 'float32')
        self.pnnx_unique_60.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_60.weight', (128,128), 'float32')
        self.pnnx_unique_61.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_61.weight', (128,128), 'float32')
        self.pnnx_unique_62.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_62.weight', (128,128), 'float32')
        self.pnnx_unique_63.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_63.bias', (128), 'float32')
        self.pnnx_unique_63.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_63.weight', (128), 'float32')
        self.pnnx_unique_64.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_64.weight', (1024,256), 'float32')
        self.pnnx_unique_66.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_66.weight', (128,1024), 'float32')
        self.pnnx_unique_67.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_67.bias', (128), 'float32')
        self.pnnx_unique_67.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_67.weight', (128), 'float32')
        self.pnnx_unique_68.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_68.weight', (128,128), 'float32')
        self.pnnx_unique_69.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_69.weight', (128,128), 'float32')
        self.pnnx_unique_70.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_70.weight', (128,128), 'float32')
        self.pnnx_unique_71.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_71.weight', (128,128), 'float32')
        self.pnnx_unique_72.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_72.bias', (128), 'float32')
        self.pnnx_unique_72.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_72.weight', (128), 'float32')
        self.pnnx_unique_73.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_73.weight', (128,128), 'float32')
        self.pnnx_unique_74.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_74.weight', (128,128), 'float32')
        self.pnnx_unique_75.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_75.weight', (128,128), 'float32')
        self.pnnx_unique_76.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_76.weight', (128,128), 'float32')
        self.pnnx_unique_77.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_77.bias', (128), 'float32')
        self.pnnx_unique_77.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_77.weight', (128), 'float32')
        self.pnnx_unique_78.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_78.weight', (1024,256), 'float32')
        self.pnnx_unique_80.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_80.weight', (128,1024), 'float32')
        self.pnnx_unique_81.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_81.bias', (128), 'float32')
        self.pnnx_unique_81.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_81.weight', (128), 'float32')
        self.pnnx_unique_82.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_82.weight', (128,128), 'float32')
        self.pnnx_unique_83.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_83.weight', (128,128), 'float32')
        self.pnnx_unique_84.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_84.weight', (128,128), 'float32')
        self.pnnx_unique_85.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_85.weight', (128,128), 'float32')
        self.pnnx_unique_86.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_86.bias', (128), 'float32')
        self.pnnx_unique_86.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_86.weight', (128), 'float32')
        self.pnnx_unique_87.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_87.weight', (128,128), 'float32')
        self.pnnx_unique_88.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_88.weight', (128,128), 'float32')
        self.pnnx_unique_89.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_89.weight', (128,128), 'float32')
        self.pnnx_unique_90.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_90.weight', (128,128), 'float32')
        self.pnnx_unique_91.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_91.bias', (128), 'float32')
        self.pnnx_unique_91.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_91.weight', (128), 'float32')
        self.pnnx_unique_92.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_92.weight', (1024,256), 'float32')
        self.pnnx_unique_94.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_94.weight', (128,1024), 'float32')
        self.pnnx_unique_95.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_95.bias', (128), 'float32')
        self.pnnx_unique_95.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_95.weight', (128), 'float32')
        self.pnnx_unique_96.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_96.bias', (128), 'float32')
        self.pnnx_unique_96.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_96.weight', (128,128), 'float32')
        self.pnnx_unique_97.bias = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_97.bias', (128), 'float32')
        self.pnnx_unique_97.weight = self.load_pnnx_bin_as_parameter(archive, 'pnnx_unique_97.weight', (128,128), 'float32')
        self.upsampler_0.bias = self.load_pnnx_bin_as_parameter(archive, 'upsampler.0.bias', (256), 'float32')
        self.upsampler_0.weight = self.load_pnnx_bin_as_parameter(archive, 'upsampler.0.weight', (256,130,3,3), 'float32')
        self.upsampler_2.bias = self.load_pnnx_bin_as_parameter(archive, 'upsampler.2.bias', (144), 'float32')
        self.upsampler_2.weight = self.load_pnnx_bin_as_parameter(archive, 'upsampler.2.weight', (144,256,1,1), 'float32')
        self.pnnx_1_pnnx_1 = self.load_pnnx_bin_as_parameter(archive, 'pnnx_1.pnnx_1', (2), 'float32')
        self.pnnx_10_pnnx_10 = self.load_pnnx_bin_as_parameter(archive, 'pnnx_10.pnnx_10', (3), 'float32')
        self.pnnx_14_pnnx_14 = self.load_pnnx_bin_as_parameter(archive, 'pnnx_14.pnnx_14', (3), 'float32')
        archive.close()

    def load_pnnx_bin_as_parameter(self, archive, key, shape, dtype, requires_grad=True):
        return nn.Parameter(self.load_pnnx_bin_as_tensor(archive, key, shape, dtype), requires_grad)

    def load_pnnx_bin_as_tensor(self, archive, key, shape, dtype):
        _, tmppath = tempfile.mkstemp()
        tmpf = open(tmppath, 'wb')
        with archive.open(key) as keyfile:
            tmpf.write(keyfile.read())
        tmpf.close()
        m = np.memmap(tmppath, dtype=dtype, mode='r', shape=shape).copy()
        os.remove(tmppath)
        return torch.from_numpy(m)

    def forward(self, v_0, v_1):
        v_2 = self.pnnx_1_pnnx_1
        v_3 = self.pnnx_10_pnnx_10
        v_4 = self.pnnx_14_pnnx_14
        v_5 = [int(v_0.size(0)), int(v_0.size(1)), int(v_0.size(2)), int(v_0.size(3))]
        v_6 = v_4.view(1, -1, 1, 1)
        v_7 = v_6.expand(*v_5)
        v_8 = v_3.view(1, -1, 1, 1)
        v_9 = v_8.expand(*v_5)
        v_10 = ((v_0 - v_7) / v_9)
        v_11 = ((v_1 - v_7) / v_9)
        v_12 = torch.cat((v_10, v_11), dim=0)
        v_13 = self.backbone_conv1(v_12)
        v_14 = self.backbone_norm1(v_13)
        v_15 = self.backbone_relu1(v_14)
        v_16 = self.backbone_layer1_0_conv1(v_15)
        v_17 = self.backbone_layer1_0_norm1(v_16)
        v_18 = self.backbone_layer1_0_relu(v_17)
        v_19 = self.backbone_layer1_0_conv2(v_18)
        v_20 = self.backbone_layer1_0_norm2(v_19)
        v_21 = self.pnnx_unique_0(v_20)
        v_22 = (v_15 + v_21)
        v_23 = self.pnnx_unique_1(v_22)
        v_24 = self.backbone_layer1_1_conv1(v_23)
        v_25 = self.backbone_layer1_1_norm1(v_24)
        v_26 = self.backbone_layer1_1_relu(v_25)
        v_27 = self.backbone_layer1_1_conv2(v_26)
        v_28 = self.backbone_layer1_1_norm2(v_27)
        v_29 = self.pnnx_unique_2(v_28)
        v_30 = (v_23 + v_29)
        v_31 = self.pnnx_unique_3(v_30)
        v_32 = self.backbone_layer2_0_conv1(v_31)
        v_33 = self.backbone_layer2_0_norm1(v_32)
        v_34 = self.backbone_layer2_0_relu(v_33)
        v_35 = self.backbone_layer2_0_conv2(v_34)
        v_36 = self.backbone_layer2_0_norm2(v_35)
        v_37 = self.pnnx_unique_4(v_36)
        v_38 = self.backbone_layer2_0_downsample_0(v_31)
        v_39 = self.backbone_layer2_0_downsample_1(v_38)
        v_40 = (v_39 + v_37)
        v_41 = self.pnnx_unique_5(v_40)
        v_42 = self.backbone_layer2_1_conv1(v_41)
        v_43 = self.backbone_layer2_1_norm1(v_42)
        v_44 = self.backbone_layer2_1_relu(v_43)
        v_45 = self.backbone_layer2_1_conv2(v_44)
        v_46 = self.backbone_layer2_1_norm2(v_45)
        v_47 = self.pnnx_unique_6(v_46)
        v_48 = (v_41 + v_47)
        v_49 = self.pnnx_unique_7(v_48)
        v_50 = self.backbone_layer3_0_conv1(v_49)
        v_51 = self.backbone_layer3_0_norm1(v_50)
        v_52 = self.backbone_layer3_0_relu(v_51)
        v_53 = self.backbone_layer3_0_conv2(v_52)
        v_54 = self.backbone_layer3_0_norm2(v_53)
        v_55 = self.pnnx_unique_8(v_54)
        v_56 = self.backbone_layer3_0_downsample_0(v_49)
        v_57 = self.backbone_layer3_0_downsample_1(v_56)
        v_58 = (v_57 + v_55)
        v_59 = self.pnnx_unique_9(v_58)
        v_60 = self.backbone_layer3_1_conv1(v_59)
        v_61 = self.backbone_layer3_1_norm1(v_60)
        v_62 = self.backbone_layer3_1_relu(v_61)
        v_63 = self.backbone_layer3_1_conv2(v_62)
        v_64 = self.backbone_layer3_1_norm2(v_63)
        v_65 = self.pnnx_unique_10(v_64)
        v_66 = (v_59 + v_65)
        v_67 = self.pnnx_unique_11(v_66)
        v_68 = self.backbone_conv2(v_67)
        v_69 = self.conv2d_0(v_68)
        v_70, v_71 = torch.chunk(input=v_69, chunks=2, dim=0)
        v_72 = self.conv2d_1(v_68)
        v_73, v_74 = torch.chunk(input=v_72, chunks=2, dim=0)
        v_75 = [int(v_70.size(0)), int(v_70.size(1)), 2, int((v_70.size(2) // 2)), 2, int((v_70.size(3) // 2))]
        v_76 = v_70.view(*v_75)
        v_77 = [int(((v_70.size(0) * 2) * 2)), int(v_70.size(1)), int((v_70.size(2) // 2)), int((v_70.size(3) // 2))]
        v_78 = [int(v_71.size(0)), int(v_71.size(1)), 2, int((v_71.size(2) // 2)), 2, int((v_71.size(3) // 2))]
        v_79 = v_71.view(*v_78)
        v_80 = [int(((v_71.size(0) * 2) * 2)), int(v_71.size(1)), int((v_71.size(2) // 2)), int((v_71.size(3) // 2))]
        v_81 = torch.permute(input=v_76, dims=(0,2,4,1,3,5))
        v_82 = v_81.reshape(*v_77)
        v_83 = [int(v_82.size(0)), int(v_82.size(2)), int(v_82.size(3))]
        v_84 = torch.ones(size=v_83)
        v_85 = torch.cumsum(input=v_84, dim=1)
        v_86 = v_85[:,-1:]
        v_87 = ((v_85 / (v_86 + 1.000000e-06)) * 6.283185e+00)
        v_88 = torch.cumsum(input=v_84, dim=2)
        v_89 = v_88[:,:,-1:]
        v_90 = ((v_88 / (v_89 + 1.000000e-06)) * 6.283185e+00)
        v_91 = torch.arange(end=64)
        v_92 = torch.pow(1.000000e+04, (((v_91 // 2) * 2) / 64))
        v_93 = torch.unsqueeze(input=v_90, dim=3)
        v_94 = (v_93 / v_92)
        v_95 = torch.unsqueeze(input=v_87, dim=3)
        v_96 = (v_95 / v_92)
        v_97 = v_94[:,:,:,::2]
        v_98 = torch.sin(v_97)
        v_99 = v_94[:,:,:,1::2]
        v_100 = torch.cos(v_99)
        v_101 = torch.stack((v_98, v_100), dim=4)
        v_102 = v_96[:,:,:,::2]
        v_103 = torch.sin(v_102)
        v_104 = v_96[:,:,:,1::2]
        v_105 = torch.cos(v_104)
        v_106 = torch.stack((v_103, v_105), dim=4)
        v_107 = torch.flatten(input=v_106, end_dim=-1, start_dim=3)
        v_108 = torch.flatten(input=v_101, end_dim=-1, start_dim=3)
        v_109 = torch.cat((v_107, v_108), dim=3)
        v_110 = torch.permute(input=v_109, dims=(0,3,1,2))
        v_111 = (v_82 + v_110)
        v_112 = torch.permute(input=v_79, dims=(0,2,4,1,3,5))
        v_113 = v_112.reshape(*v_80)
        v_114 = (v_113 + v_110)
        v_115 = [int(((v_111.size(0) // 2) // 2)), 2, 2, int(v_111.size(1)), int(v_111.size(2)), int(v_111.size(3))]
        v_116 = v_111.view(*v_115)
        v_117 = [int(((v_111.size(0) // 2) // 2)), int(v_111.size(1)), int((v_111.size(2) * 2)), int((v_111.size(3) * 2))]
        v_118 = torch.permute(input=v_116, dims=(0,3,1,4,2,5))
        v_119 = [int(((v_114.size(0) // 2) // 2)), 2, 2, int(v_114.size(1)), int(v_114.size(2)), int(v_114.size(3))]
        v_120 = v_114.view(*v_119)
        v_121 = [int(((v_114.size(0) // 2) // 2)), int(v_114.size(1)), int((v_114.size(2) * 2)), int((v_114.size(3) * 2))]
        v_122 = torch.permute(input=v_120, dims=(0,3,1,4,2,5))
        v_123 = v_118.reshape(*v_117)
        v_124 = torch.flatten(input=v_123, end_dim=-1, start_dim=-2)
        v_125 = v_122.reshape(*v_121)
        v_126 = torch.flatten(input=v_125, end_dim=-1, start_dim=-2)
        v_127 = [int((v_123.size(2) - (v_123.size(2) // 2))), int((v_123.size(3) - (v_123.size(3) // 2)))]
        v_128 = torch.ones(size=v_127)
        v_129 = [int((v_123.size(2) - (v_123.size(2) // 2))), int(((v_123.size(3) // 2) - ((v_123.size(3) // 2) // 2)))]
        v_130 = torch.ones(size=v_129)
        v_131 = (v_130 * 2)
        v_132 = [int((v_123.size(2) - (v_123.size(2) // 2))), int(((v_123.size(3) // 2) // 2))]
        v_133 = torch.ones(size=v_132)
        v_134 = (v_133 * 3)
        v_135 = [int(((v_123.size(2) // 2) - ((v_123.size(2) // 2) // 2))), int((v_123.size(3) - (v_123.size(3) // 2)))]
        v_136 = torch.ones(size=v_135)
        v_137 = (v_136 * 4)
        v_138 = [int(((v_123.size(2) // 2) - ((v_123.size(2) // 2) // 2))), int(((v_123.size(3) // 2) - ((v_123.size(3) // 2) // 2)))]
        v_139 = torch.ones(size=v_138)
        v_140 = (v_139 * 5)
        v_141 = [int(((v_123.size(2) // 2) - ((v_123.size(2) // 2) // 2))), int(((v_123.size(3) // 2) // 2))]
        v_142 = torch.ones(size=v_141)
        v_143 = (v_142 * 6)
        v_144 = [int(((v_123.size(2) // 2) // 2)), int((v_123.size(3) - (v_123.size(3) // 2)))]
        v_145 = torch.ones(size=v_144)
        v_146 = (v_145 * 7)
        v_147 = [int(((v_123.size(2) // 2) // 2)), int(((v_123.size(3) // 2) - ((v_123.size(3) // 2) // 2)))]
        v_148 = torch.ones(size=v_147)
        v_149 = (v_148 * 8)
        v_150 = [int(((v_123.size(2) // 2) // 2)), int(((v_123.size(3) // 2) // 2))]
        v_151 = torch.ones(size=v_150)
        v_152 = (v_151 * 9)
        v_153 = torch.cat((v_146, v_149, v_152), dim=1)
        v_154 = torch.cat((v_137, v_140, v_143), dim=1)
        v_155 = torch.cat((v_128, v_131, v_134), dim=1)
        v_156 = torch.cat((v_155, v_154, v_153), dim=0)
        v_157 = torch.unsqueeze(input=v_156, dim=0)
        v_158 = torch.unsqueeze(input=v_157, dim=-1)
        v_159 = [int(v_158.size(0)), int((v_123.size(3) // (v_123.size(3) // 2))), int((v_158.size(1) // (v_123.size(3) // (v_123.size(3) // 2)))), int((v_123.size(3) // (v_123.size(3) // 2))), int((v_158.size(2) // (v_123.size(3) // (v_123.size(3) // 2)))), int(v_158.size(3))]
        v_160 = v_158.view(*v_159)
        v_161 = [-1, int(((v_123.size(2) // 2) * (v_123.size(3) // 2)))]
        v_162 = torch.permute(input=v_160, dims=(0,1,3,2,4,5))
        v_163 = v_162.reshape(*v_161)
        v_164 = torch.unsqueeze(input=v_163, dim=2)
        v_165 = torch.unsqueeze(input=v_163, dim=1)
        v_166 = (v_165 - v_164)
        v_167 = torch.permute(input=v_126, dims=(0,2,1))
        v_168 = torch.permute(input=v_124, dims=(0,2,1))
        v_169 = torch.cat((v_168, v_167), dim=0)
        v_170 = self.transformer_layers_0_self_attn_q_proj(v_169)
        v_171 = self.transformer_layers_0_self_attn_k_proj(v_169)
        v_172 = self.transformer_layers_0_self_attn_v_proj(v_169)
        v_173 = [int(v_170.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_170.size(2))]
        v_174 = v_170.view(*v_173)
        v_175 = [int(v_174.size(0)), 2, int((v_174.size(1) // 2)), 2, int((v_174.size(2) // 2)), int(v_174.size(3))]
        v_176 = v_174.view(*v_175)
        v_177 = v_171.view(*v_173)
        v_178 = [int(v_177.size(0)), 2, int((v_177.size(1) // 2)), 2, int((v_177.size(2) // 2)), int(v_177.size(3))]
        v_179 = v_177.view(*v_178)
        v_180 = v_172.view(*v_173)
        v_181 = [int(v_180.size(0)), 2, int((v_180.size(1) // 2)), 2, int((v_180.size(2) // 2)), int(v_180.size(3))]
        v_182 = v_180.view(*v_181)
        v_183 = [int(((v_170.size(0) * 2) * 2)), -1, int(v_170.size(2))]
        v_184 = torch.permute(input=v_176, dims=(0,1,3,2,4,5))
        v_185 = torch.permute(input=v_179, dims=(0,1,3,2,4,5))
        v_186 = v_185.reshape(*v_183)
        v_187 = v_184.reshape(*v_183)
        v_188 = torch.permute(input=v_186, dims=(0,2,1))
        v_189 = torch.matmul(input=v_187, other=v_188)
        v_190 = (v_189 / torch.pow(v_170.size(2), 5.000000e-01))
        v_191 = torch.permute(input=v_182, dims=(0,1,3,2,4,5))
        v_192 = F.softmax(input=v_190, dim=-1)
        v_193 = v_191.reshape(*v_183)
        v_194 = [int(((v_170.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_170.size(2))]
        v_195 = torch.matmul(input=v_192, other=v_193)
        v_196 = v_195.view(*v_194)
        v_197 = [int(((v_196.size(0) // 2) // 2)), 2, 2, int(v_196.size(1)), int(v_196.size(2)), int(v_196.size(3))]
        v_198 = v_196.view(*v_197)
        v_199 = torch.permute(input=v_198, dims=(0,1,3,2,4,5))
        v_200 = [int(v_170.size(0)), -1, int(v_170.size(2))]
        v_201 = v_199.reshape(*v_200)
        v_202 = self.transformer_layers_0_self_attn_merge(v_201)
        v_203 = self.transformer_layers_0_self_attn_norm1(v_202)
        v_204 = (v_169 + v_203)
        v_205 = self.transformer_layers_0_cross_attn_ffn_q_proj(v_204)
        v_206 = torch.cat((v_167, v_168), dim=0)
        v_207 = self.transformer_layers_0_cross_attn_ffn_k_proj(v_206)
        v_208 = self.transformer_layers_0_cross_attn_ffn_v_proj(v_206)
        v_209 = [int(v_205.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_205.size(2))]
        v_210 = v_205.view(*v_209)
        v_211 = [int(v_210.size(0)), 2, int((v_210.size(1) // 2)), 2, int((v_210.size(2) // 2)), int(v_210.size(3))]
        v_212 = v_210.view(*v_211)
        v_213 = v_207.view(*v_209)
        v_214 = [int(v_213.size(0)), 2, int((v_213.size(1) // 2)), 2, int((v_213.size(2) // 2)), int(v_213.size(3))]
        v_215 = v_213.view(*v_214)
        v_216 = v_208.view(*v_209)
        v_217 = [int(v_216.size(0)), 2, int((v_216.size(1) // 2)), 2, int((v_216.size(2) // 2)), int(v_216.size(3))]
        v_218 = v_216.view(*v_217)
        v_219 = [int(((v_205.size(0) * 2) * 2)), -1, int(v_205.size(2))]
        v_220 = torch.permute(input=v_212, dims=(0,1,3,2,4,5))
        v_221 = torch.permute(input=v_215, dims=(0,1,3,2,4,5))
        v_222 = v_221.reshape(*v_219)
        v_223 = v_220.reshape(*v_219)
        v_224 = torch.permute(input=v_222, dims=(0,2,1))
        v_225 = torch.matmul(input=v_223, other=v_224)
        v_226 = (v_225 / torch.pow(v_205.size(2), 5.000000e-01))
        v_227 = torch.permute(input=v_218, dims=(0,1,3,2,4,5))
        v_228 = F.softmax(input=v_226, dim=-1)
        v_229 = v_227.reshape(*v_219)
        v_230 = [int(((v_205.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_205.size(2))]
        v_231 = torch.matmul(input=v_228, other=v_229)
        v_232 = v_231.view(*v_230)
        v_233 = [int(((v_232.size(0) // 2) // 2)), 2, 2, int(v_232.size(1)), int(v_232.size(2)), int(v_232.size(3))]
        v_234 = v_232.view(*v_233)
        v_235 = torch.permute(input=v_234, dims=(0,1,3,2,4,5))
        v_236 = [int(v_205.size(0)), -1, int(v_205.size(2))]
        v_237 = v_235.reshape(*v_236)
        v_238 = self.transformer_layers_0_cross_attn_ffn_merge(v_237)
        v_239 = self.transformer_layers_0_cross_attn_ffn_norm1(v_238)
        v_240 = torch.cat((v_204, v_239), dim=-1)
        v_241 = self.transformer_layers_0_cross_attn_ffn_mlp_0(v_240)
        v_242 = self.transformer_layers_0_cross_attn_ffn_mlp_1(v_241)
        v_243 = self.transformer_layers_0_cross_attn_ffn_mlp_2(v_242)
        v_244 = self.transformer_layers_0_cross_attn_ffn_norm2(v_243)
        v_245 = (v_204 + v_244)
        v_246, v_247 = torch.chunk(input=v_245, chunks=2, dim=0)
        v_248 = self.transformer_layers_1_self_attn_q_proj(v_245)
        v_249 = self.transformer_layers_1_self_attn_k_proj(v_245)
        v_250 = self.transformer_layers_1_self_attn_v_proj(v_245)
        v_251 = [int(v_248.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_248.size(2))]
        v_252 = [int(torch.neg(((v_123.size(2) // 2) // 2))), int(torch.neg(((v_123.size(3) // 2) // 2)))]
        v_253 = v_248.view(*v_251)
        v_254 = torch.roll(input=v_253, shifts=v_252, dims=(1,2))
        v_255 = [int(v_254.size(0)), 2, int((v_254.size(1) // 2)), 2, int((v_254.size(2) // 2)), int(v_254.size(3))]
        v_256 = v_254.view(*v_255)
        v_257 = v_249.view(*v_251)
        v_258 = torch.roll(input=v_257, shifts=v_252, dims=(1,2))
        v_259 = [int(v_258.size(0)), 2, int((v_258.size(1) // 2)), 2, int((v_258.size(2) // 2)), int(v_258.size(3))]
        v_260 = v_258.view(*v_259)
        v_261 = v_250.view(*v_251)
        v_262 = torch.roll(input=v_261, shifts=v_252, dims=(1,2))
        v_263 = [int(v_262.size(0)), 2, int((v_262.size(1) // 2)), 2, int((v_262.size(2) // 2)), int(v_262.size(3))]
        v_264 = v_262.view(*v_263)
        v_265 = [int(((v_248.size(0) * 2) * 2)), -1, int(v_248.size(2))]
        v_266 = torch.permute(input=v_256, dims=(0,1,3,2,4,5))
        v_267 = torch.permute(input=v_260, dims=(0,1,3,2,4,5))
        v_268 = v_267.reshape(*v_265)
        v_269 = v_266.reshape(*v_265)
        v_270 = torch.permute(input=v_268, dims=(0,2,1))
        v_271 = torch.matmul(input=v_269, other=v_270)
        v_272 = [int(v_248.size(0)), 1, 1]
        v_273 = torch.ne(input=v_166, other=0)
        v_274 = v_166.masked_fill(v_273, value=-100.000000)
        v_275 = torch.eq(input=v_166, other=0)
        v_276 = v_274.masked_fill(v_275, value=0.000000)
        v_277 = v_276.repeat(*v_272)
        v_278 = ((v_271 / torch.pow(v_248.size(2), 5.000000e-01)) + v_277)
        v_279 = torch.permute(input=v_264, dims=(0,1,3,2,4,5))
        v_280 = F.softmax(input=v_278, dim=-1)
        v_281 = v_279.reshape(*v_265)
        v_282 = [int(((v_248.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_248.size(2))]
        v_283 = torch.matmul(input=v_280, other=v_281)
        v_284 = v_283.view(*v_282)
        v_285 = [int(((v_284.size(0) // 2) // 2)), 2, 2, int(v_284.size(1)), int(v_284.size(2)), int(v_284.size(3))]
        v_286 = v_284.view(*v_285)
        v_287 = [int(((v_284.size(0) // 2) // 2)), int((v_284.size(1) * 2)), int((v_284.size(2) * 2)), int(v_284.size(3))]
        v_288 = torch.permute(input=v_286, dims=(0,1,3,2,4,5))
        v_289 = [int(v_248.size(0)), -1, int(v_248.size(2))]
        v_290 = v_288.reshape(*v_287)
        v_291 = torch.roll(input=v_290, shifts=v_150, dims=(1,2))
        v_292 = v_291.view(*v_289)
        v_293 = self.transformer_layers_1_self_attn_merge(v_292)
        v_294 = self.transformer_layers_1_self_attn_norm1(v_293)
        v_295 = (v_245 + v_294)
        v_296 = self.transformer_layers_1_cross_attn_ffn_q_proj(v_295)
        v_297 = torch.cat((v_247, v_246), dim=0)
        v_298 = self.transformer_layers_1_cross_attn_ffn_k_proj(v_297)
        v_299 = self.transformer_layers_1_cross_attn_ffn_v_proj(v_297)
        v_300 = [int(v_296.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_296.size(2))]
        v_301 = v_296.view(*v_300)
        v_302 = torch.roll(input=v_301, shifts=v_252, dims=(1,2))
        v_303 = [int(v_302.size(0)), 2, int((v_302.size(1) // 2)), 2, int((v_302.size(2) // 2)), int(v_302.size(3))]
        v_304 = v_302.view(*v_303)
        v_305 = v_298.view(*v_300)
        v_306 = torch.roll(input=v_305, shifts=v_252, dims=(1,2))
        v_307 = [int(v_306.size(0)), 2, int((v_306.size(1) // 2)), 2, int((v_306.size(2) // 2)), int(v_306.size(3))]
        v_308 = v_306.view(*v_307)
        v_309 = v_299.view(*v_300)
        v_310 = torch.roll(input=v_309, shifts=v_252, dims=(1,2))
        v_311 = [int(v_310.size(0)), 2, int((v_310.size(1) // 2)), 2, int((v_310.size(2) // 2)), int(v_310.size(3))]
        v_312 = v_310.view(*v_311)
        v_313 = [int(((v_296.size(0) * 2) * 2)), -1, int(v_296.size(2))]
        v_314 = torch.permute(input=v_304, dims=(0,1,3,2,4,5))
        v_315 = torch.permute(input=v_308, dims=(0,1,3,2,4,5))
        v_316 = v_315.reshape(*v_313)
        v_317 = v_314.reshape(*v_313)
        v_318 = torch.permute(input=v_316, dims=(0,2,1))
        v_319 = torch.matmul(input=v_317, other=v_318)
        v_320 = [int(v_296.size(0)), 1, 1]
        v_321 = v_276.repeat(*v_320)
        v_322 = ((v_319 / torch.pow(v_296.size(2), 5.000000e-01)) + v_321)
        v_323 = torch.permute(input=v_312, dims=(0,1,3,2,4,5))
        v_324 = F.softmax(input=v_322, dim=-1)
        v_325 = v_323.reshape(*v_313)
        v_326 = [int(((v_296.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_296.size(2))]
        v_327 = torch.matmul(input=v_324, other=v_325)
        v_328 = v_327.view(*v_326)
        v_329 = [int(((v_328.size(0) // 2) // 2)), 2, 2, int(v_328.size(1)), int(v_328.size(2)), int(v_328.size(3))]
        v_330 = v_328.view(*v_329)
        v_331 = [int(((v_328.size(0) // 2) // 2)), int((v_328.size(1) * 2)), int((v_328.size(2) * 2)), int(v_328.size(3))]
        v_332 = torch.permute(input=v_330, dims=(0,1,3,2,4,5))
        v_333 = [int(v_296.size(0)), -1, int(v_296.size(2))]
        v_334 = v_332.reshape(*v_331)
        v_335 = torch.roll(input=v_334, shifts=v_150, dims=(1,2))
        v_336 = v_335.view(*v_333)
        v_337 = self.transformer_layers_1_cross_attn_ffn_merge(v_336)
        v_338 = self.transformer_layers_1_cross_attn_ffn_norm1(v_337)
        v_339 = torch.cat((v_295, v_338), dim=-1)
        v_340 = self.transformer_layers_1_cross_attn_ffn_mlp_0(v_339)
        v_341 = self.transformer_layers_1_cross_attn_ffn_mlp_1(v_340)
        v_342 = self.transformer_layers_1_cross_attn_ffn_mlp_2(v_341)
        v_343 = self.transformer_layers_1_cross_attn_ffn_norm2(v_342)
        v_344 = (v_295 + v_343)
        v_345, v_346 = torch.chunk(input=v_344, chunks=2, dim=0)
        v_347 = self.transformer_layers_2_self_attn_q_proj(v_344)
        v_348 = self.transformer_layers_2_self_attn_k_proj(v_344)
        v_349 = self.transformer_layers_2_self_attn_v_proj(v_344)
        v_350 = [int(v_347.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_347.size(2))]
        v_351 = v_347.view(*v_350)
        v_352 = [int(v_351.size(0)), 2, int((v_351.size(1) // 2)), 2, int((v_351.size(2) // 2)), int(v_351.size(3))]
        v_353 = v_351.view(*v_352)
        v_354 = v_348.view(*v_350)
        v_355 = [int(v_354.size(0)), 2, int((v_354.size(1) // 2)), 2, int((v_354.size(2) // 2)), int(v_354.size(3))]
        v_356 = v_354.view(*v_355)
        v_357 = v_349.view(*v_350)
        v_358 = [int(v_357.size(0)), 2, int((v_357.size(1) // 2)), 2, int((v_357.size(2) // 2)), int(v_357.size(3))]
        v_359 = v_357.view(*v_358)
        v_360 = [int(((v_347.size(0) * 2) * 2)), -1, int(v_347.size(2))]
        v_361 = torch.permute(input=v_353, dims=(0,1,3,2,4,5))
        v_362 = torch.permute(input=v_356, dims=(0,1,3,2,4,5))
        v_363 = v_362.reshape(*v_360)
        v_364 = v_361.reshape(*v_360)
        v_365 = torch.permute(input=v_363, dims=(0,2,1))
        v_366 = torch.matmul(input=v_364, other=v_365)
        v_367 = (v_366 / torch.pow(v_347.size(2), 5.000000e-01))
        v_368 = torch.permute(input=v_359, dims=(0,1,3,2,4,5))
        v_369 = F.softmax(input=v_367, dim=-1)
        v_370 = v_368.reshape(*v_360)
        v_371 = [int(((v_347.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_347.size(2))]
        v_372 = torch.matmul(input=v_369, other=v_370)
        v_373 = v_372.view(*v_371)
        v_374 = [int(((v_373.size(0) // 2) // 2)), 2, 2, int(v_373.size(1)), int(v_373.size(2)), int(v_373.size(3))]
        v_375 = v_373.view(*v_374)
        v_376 = torch.permute(input=v_375, dims=(0,1,3,2,4,5))
        v_377 = [int(v_347.size(0)), -1, int(v_347.size(2))]
        v_378 = v_376.reshape(*v_377)
        v_379 = self.transformer_layers_2_self_attn_merge(v_378)
        v_380 = self.transformer_layers_2_self_attn_norm1(v_379)
        v_381 = (v_344 + v_380)
        v_382 = self.transformer_layers_2_cross_attn_ffn_q_proj(v_381)
        v_383 = torch.cat((v_346, v_345), dim=0)
        v_384 = self.transformer_layers_2_cross_attn_ffn_k_proj(v_383)
        v_385 = self.transformer_layers_2_cross_attn_ffn_v_proj(v_383)
        v_386 = [int(v_382.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_382.size(2))]
        v_387 = v_382.view(*v_386)
        v_388 = [int(v_387.size(0)), 2, int((v_387.size(1) // 2)), 2, int((v_387.size(2) // 2)), int(v_387.size(3))]
        v_389 = v_387.view(*v_388)
        v_390 = v_384.view(*v_386)
        v_391 = [int(v_390.size(0)), 2, int((v_390.size(1) // 2)), 2, int((v_390.size(2) // 2)), int(v_390.size(3))]
        v_392 = v_390.view(*v_391)
        v_393 = v_385.view(*v_386)
        v_394 = [int(v_393.size(0)), 2, int((v_393.size(1) // 2)), 2, int((v_393.size(2) // 2)), int(v_393.size(3))]
        v_395 = v_393.view(*v_394)
        v_396 = [int(((v_382.size(0) * 2) * 2)), -1, int(v_382.size(2))]
        v_397 = torch.permute(input=v_389, dims=(0,1,3,2,4,5))
        v_398 = torch.permute(input=v_392, dims=(0,1,3,2,4,5))
        v_399 = v_398.reshape(*v_396)
        v_400 = v_397.reshape(*v_396)
        v_401 = torch.permute(input=v_399, dims=(0,2,1))
        v_402 = torch.matmul(input=v_400, other=v_401)
        v_403 = (v_402 / torch.pow(v_382.size(2), 5.000000e-01))
        v_404 = torch.permute(input=v_395, dims=(0,1,3,2,4,5))
        v_405 = F.softmax(input=v_403, dim=-1)
        v_406 = v_404.reshape(*v_396)
        v_407 = [int(((v_382.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_382.size(2))]
        v_408 = torch.matmul(input=v_405, other=v_406)
        v_409 = v_408.view(*v_407)
        v_410 = [int(((v_409.size(0) // 2) // 2)), 2, 2, int(v_409.size(1)), int(v_409.size(2)), int(v_409.size(3))]
        v_411 = v_409.view(*v_410)
        v_412 = torch.permute(input=v_411, dims=(0,1,3,2,4,5))
        v_413 = [int(v_382.size(0)), -1, int(v_382.size(2))]
        v_414 = v_412.reshape(*v_413)
        v_415 = self.transformer_layers_2_cross_attn_ffn_merge(v_414)
        v_416 = self.transformer_layers_2_cross_attn_ffn_norm1(v_415)
        v_417 = torch.cat((v_381, v_416), dim=-1)
        v_418 = self.transformer_layers_2_cross_attn_ffn_mlp_0(v_417)
        v_419 = self.transformer_layers_2_cross_attn_ffn_mlp_1(v_418)
        v_420 = self.transformer_layers_2_cross_attn_ffn_mlp_2(v_419)
        v_421 = self.transformer_layers_2_cross_attn_ffn_norm2(v_420)
        v_422 = (v_381 + v_421)
        v_423, v_424 = torch.chunk(input=v_422, chunks=2, dim=0)
        v_425 = self.transformer_layers_3_self_attn_q_proj(v_422)
        v_426 = self.transformer_layers_3_self_attn_k_proj(v_422)
        v_427 = self.transformer_layers_3_self_attn_v_proj(v_422)
        v_428 = [int(v_425.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_425.size(2))]
        v_429 = v_425.view(*v_428)
        v_430 = torch.roll(input=v_429, shifts=v_252, dims=(1,2))
        v_431 = [int(v_430.size(0)), 2, int((v_430.size(1) // 2)), 2, int((v_430.size(2) // 2)), int(v_430.size(3))]
        v_432 = v_430.view(*v_431)
        v_433 = v_426.view(*v_428)
        v_434 = torch.roll(input=v_433, shifts=v_252, dims=(1,2))
        v_435 = [int(v_434.size(0)), 2, int((v_434.size(1) // 2)), 2, int((v_434.size(2) // 2)), int(v_434.size(3))]
        v_436 = v_434.view(*v_435)
        v_437 = v_427.view(*v_428)
        v_438 = torch.roll(input=v_437, shifts=v_252, dims=(1,2))
        v_439 = [int(v_438.size(0)), 2, int((v_438.size(1) // 2)), 2, int((v_438.size(2) // 2)), int(v_438.size(3))]
        v_440 = v_438.view(*v_439)
        v_441 = [int(((v_425.size(0) * 2) * 2)), -1, int(v_425.size(2))]
        v_442 = torch.permute(input=v_432, dims=(0,1,3,2,4,5))
        v_443 = torch.permute(input=v_436, dims=(0,1,3,2,4,5))
        v_444 = v_443.reshape(*v_441)
        v_445 = v_442.reshape(*v_441)
        v_446 = torch.permute(input=v_444, dims=(0,2,1))
        v_447 = torch.matmul(input=v_445, other=v_446)
        v_448 = [int(v_425.size(0)), 1, 1]
        v_449 = v_276.repeat(*v_448)
        v_450 = ((v_447 / torch.pow(v_425.size(2), 5.000000e-01)) + v_449)
        v_451 = torch.permute(input=v_440, dims=(0,1,3,2,4,5))
        v_452 = F.softmax(input=v_450, dim=-1)
        v_453 = v_451.reshape(*v_441)
        v_454 = [int(((v_425.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_425.size(2))]
        v_455 = torch.matmul(input=v_452, other=v_453)
        v_456 = v_455.view(*v_454)
        v_457 = [int(((v_456.size(0) // 2) // 2)), 2, 2, int(v_456.size(1)), int(v_456.size(2)), int(v_456.size(3))]
        v_458 = v_456.view(*v_457)
        v_459 = [int(((v_456.size(0) // 2) // 2)), int((v_456.size(1) * 2)), int((v_456.size(2) * 2)), int(v_456.size(3))]
        v_460 = torch.permute(input=v_458, dims=(0,1,3,2,4,5))
        v_461 = [int(v_425.size(0)), -1, int(v_425.size(2))]
        v_462 = v_460.reshape(*v_459)
        v_463 = torch.roll(input=v_462, shifts=v_150, dims=(1,2))
        v_464 = v_463.view(*v_461)
        v_465 = self.transformer_layers_3_self_attn_merge(v_464)
        v_466 = self.transformer_layers_3_self_attn_norm1(v_465)
        v_467 = (v_422 + v_466)
        v_468 = self.transformer_layers_3_cross_attn_ffn_q_proj(v_467)
        v_469 = torch.cat((v_424, v_423), dim=0)
        v_470 = self.transformer_layers_3_cross_attn_ffn_k_proj(v_469)
        v_471 = self.transformer_layers_3_cross_attn_ffn_v_proj(v_469)
        v_472 = [int(v_468.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_468.size(2))]
        v_473 = v_468.view(*v_472)
        v_474 = torch.roll(input=v_473, shifts=v_252, dims=(1,2))
        v_475 = [int(v_474.size(0)), 2, int((v_474.size(1) // 2)), 2, int((v_474.size(2) // 2)), int(v_474.size(3))]
        v_476 = v_474.view(*v_475)
        v_477 = v_470.view(*v_472)
        v_478 = torch.roll(input=v_477, shifts=v_252, dims=(1,2))
        v_479 = [int(v_478.size(0)), 2, int((v_478.size(1) // 2)), 2, int((v_478.size(2) // 2)), int(v_478.size(3))]
        v_480 = v_478.view(*v_479)
        v_481 = v_471.view(*v_472)
        v_482 = torch.roll(input=v_481, shifts=v_252, dims=(1,2))
        v_483 = [int(v_482.size(0)), 2, int((v_482.size(1) // 2)), 2, int((v_482.size(2) // 2)), int(v_482.size(3))]
        v_484 = v_482.view(*v_483)
        v_485 = [int(((v_468.size(0) * 2) * 2)), -1, int(v_468.size(2))]
        v_486 = torch.permute(input=v_476, dims=(0,1,3,2,4,5))
        v_487 = torch.permute(input=v_480, dims=(0,1,3,2,4,5))
        v_488 = v_487.reshape(*v_485)
        v_489 = v_486.reshape(*v_485)
        v_490 = torch.permute(input=v_488, dims=(0,2,1))
        v_491 = torch.matmul(input=v_489, other=v_490)
        v_492 = [int(v_468.size(0)), 1, 1]
        v_493 = v_276.repeat(*v_492)
        v_494 = ((v_491 / torch.pow(v_468.size(2), 5.000000e-01)) + v_493)
        v_495 = torch.permute(input=v_484, dims=(0,1,3,2,4,5))
        v_496 = F.softmax(input=v_494, dim=-1)
        v_497 = v_495.reshape(*v_485)
        v_498 = [int(((v_468.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_468.size(2))]
        v_499 = torch.matmul(input=v_496, other=v_497)
        v_500 = v_499.view(*v_498)
        v_501 = [int(((v_500.size(0) // 2) // 2)), 2, 2, int(v_500.size(1)), int(v_500.size(2)), int(v_500.size(3))]
        v_502 = v_500.view(*v_501)
        v_503 = [int(((v_500.size(0) // 2) // 2)), int((v_500.size(1) * 2)), int((v_500.size(2) * 2)), int(v_500.size(3))]
        v_504 = torch.permute(input=v_502, dims=(0,1,3,2,4,5))
        v_505 = [int(v_468.size(0)), -1, int(v_468.size(2))]
        v_506 = v_504.reshape(*v_503)
        v_507 = torch.roll(input=v_506, shifts=v_150, dims=(1,2))
        v_508 = v_507.view(*v_505)
        v_509 = self.transformer_layers_3_cross_attn_ffn_merge(v_508)
        v_510 = self.transformer_layers_3_cross_attn_ffn_norm1(v_509)
        v_511 = torch.cat((v_467, v_510), dim=-1)
        v_512 = self.transformer_layers_3_cross_attn_ffn_mlp_0(v_511)
        v_513 = self.transformer_layers_3_cross_attn_ffn_mlp_1(v_512)
        v_514 = self.transformer_layers_3_cross_attn_ffn_mlp_2(v_513)
        v_515 = self.transformer_layers_3_cross_attn_ffn_norm2(v_514)
        v_516 = (v_467 + v_515)
        v_517, v_518 = torch.chunk(input=v_516, chunks=2, dim=0)
        v_519 = self.transformer_layers_4_self_attn_q_proj(v_516)
        v_520 = self.transformer_layers_4_self_attn_k_proj(v_516)
        v_521 = self.transformer_layers_4_self_attn_v_proj(v_516)
        v_522 = [int(v_519.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_519.size(2))]
        v_523 = v_519.view(*v_522)
        v_524 = [int(v_523.size(0)), 2, int((v_523.size(1) // 2)), 2, int((v_523.size(2) // 2)), int(v_523.size(3))]
        v_525 = v_523.view(*v_524)
        v_526 = v_520.view(*v_522)
        v_527 = [int(v_526.size(0)), 2, int((v_526.size(1) // 2)), 2, int((v_526.size(2) // 2)), int(v_526.size(3))]
        v_528 = v_526.view(*v_527)
        v_529 = v_521.view(*v_522)
        v_530 = [int(v_529.size(0)), 2, int((v_529.size(1) // 2)), 2, int((v_529.size(2) // 2)), int(v_529.size(3))]
        v_531 = v_529.view(*v_530)
        v_532 = [int(((v_519.size(0) * 2) * 2)), -1, int(v_519.size(2))]
        v_533 = torch.permute(input=v_525, dims=(0,1,3,2,4,5))
        v_534 = torch.permute(input=v_528, dims=(0,1,3,2,4,5))
        v_535 = v_534.reshape(*v_532)
        v_536 = v_533.reshape(*v_532)
        v_537 = torch.permute(input=v_535, dims=(0,2,1))
        v_538 = torch.matmul(input=v_536, other=v_537)
        v_539 = (v_538 / torch.pow(v_519.size(2), 5.000000e-01))
        v_540 = torch.permute(input=v_531, dims=(0,1,3,2,4,5))
        v_541 = F.softmax(input=v_539, dim=-1)
        v_542 = v_540.reshape(*v_532)
        v_543 = [int(((v_519.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_519.size(2))]
        v_544 = torch.matmul(input=v_541, other=v_542)
        v_545 = v_544.view(*v_543)
        v_546 = [int(((v_545.size(0) // 2) // 2)), 2, 2, int(v_545.size(1)), int(v_545.size(2)), int(v_545.size(3))]
        v_547 = v_545.view(*v_546)
        v_548 = torch.permute(input=v_547, dims=(0,1,3,2,4,5))
        v_549 = [int(v_519.size(0)), -1, int(v_519.size(2))]
        v_550 = v_548.reshape(*v_549)
        v_551 = self.transformer_layers_4_self_attn_merge(v_550)
        v_552 = self.transformer_layers_4_self_attn_norm1(v_551)
        v_553 = (v_516 + v_552)
        v_554 = self.transformer_layers_4_cross_attn_ffn_q_proj(v_553)
        v_555 = torch.cat((v_518, v_517), dim=0)
        v_556 = self.transformer_layers_4_cross_attn_ffn_k_proj(v_555)
        v_557 = self.transformer_layers_4_cross_attn_ffn_v_proj(v_555)
        v_558 = [int(v_554.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_554.size(2))]
        v_559 = v_554.view(*v_558)
        v_560 = [int(v_559.size(0)), 2, int((v_559.size(1) // 2)), 2, int((v_559.size(2) // 2)), int(v_559.size(3))]
        v_561 = v_559.view(*v_560)
        v_562 = v_556.view(*v_558)
        v_563 = [int(v_562.size(0)), 2, int((v_562.size(1) // 2)), 2, int((v_562.size(2) // 2)), int(v_562.size(3))]
        v_564 = v_562.view(*v_563)
        v_565 = v_557.view(*v_558)
        v_566 = [int(v_565.size(0)), 2, int((v_565.size(1) // 2)), 2, int((v_565.size(2) // 2)), int(v_565.size(3))]
        v_567 = v_565.view(*v_566)
        v_568 = [int(((v_554.size(0) * 2) * 2)), -1, int(v_554.size(2))]
        v_569 = torch.permute(input=v_561, dims=(0,1,3,2,4,5))
        v_570 = torch.permute(input=v_564, dims=(0,1,3,2,4,5))
        v_571 = v_570.reshape(*v_568)
        v_572 = v_569.reshape(*v_568)
        v_573 = torch.permute(input=v_571, dims=(0,2,1))
        v_574 = torch.matmul(input=v_572, other=v_573)
        v_575 = (v_574 / torch.pow(v_554.size(2), 5.000000e-01))
        v_576 = torch.permute(input=v_567, dims=(0,1,3,2,4,5))
        v_577 = F.softmax(input=v_575, dim=-1)
        v_578 = v_576.reshape(*v_568)
        v_579 = [int(((v_554.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_554.size(2))]
        v_580 = torch.matmul(input=v_577, other=v_578)
        v_581 = v_580.view(*v_579)
        v_582 = [int(((v_581.size(0) // 2) // 2)), 2, 2, int(v_581.size(1)), int(v_581.size(2)), int(v_581.size(3))]
        v_583 = v_581.view(*v_582)
        v_584 = torch.permute(input=v_583, dims=(0,1,3,2,4,5))
        v_585 = [int(v_554.size(0)), -1, int(v_554.size(2))]
        v_586 = v_584.reshape(*v_585)
        v_587 = self.transformer_layers_4_cross_attn_ffn_merge(v_586)
        v_588 = self.transformer_layers_4_cross_attn_ffn_norm1(v_587)
        v_589 = torch.cat((v_553, v_588), dim=-1)
        v_590 = self.transformer_layers_4_cross_attn_ffn_mlp_0(v_589)
        v_591 = self.transformer_layers_4_cross_attn_ffn_mlp_1(v_590)
        v_592 = self.transformer_layers_4_cross_attn_ffn_mlp_2(v_591)
        v_593 = self.transformer_layers_4_cross_attn_ffn_norm2(v_592)
        v_594 = (v_553 + v_593)
        v_595, v_596 = torch.chunk(input=v_594, chunks=2, dim=0)
        v_597 = self.transformer_layers_5_self_attn_q_proj(v_594)
        v_598 = self.transformer_layers_5_self_attn_k_proj(v_594)
        v_599 = self.transformer_layers_5_self_attn_v_proj(v_594)
        v_600 = [int(v_597.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_597.size(2))]
        v_601 = v_597.view(*v_600)
        v_602 = torch.roll(input=v_601, shifts=v_252, dims=(1,2))
        v_603 = [int(v_602.size(0)), 2, int((v_602.size(1) // 2)), 2, int((v_602.size(2) // 2)), int(v_602.size(3))]
        v_604 = v_602.view(*v_603)
        v_605 = v_598.view(*v_600)
        v_606 = torch.roll(input=v_605, shifts=v_252, dims=(1,2))
        v_607 = [int(v_606.size(0)), 2, int((v_606.size(1) // 2)), 2, int((v_606.size(2) // 2)), int(v_606.size(3))]
        v_608 = v_606.view(*v_607)
        v_609 = v_599.view(*v_600)
        v_610 = torch.roll(input=v_609, shifts=v_252, dims=(1,2))
        v_611 = [int(v_610.size(0)), 2, int((v_610.size(1) // 2)), 2, int((v_610.size(2) // 2)), int(v_610.size(3))]
        v_612 = v_610.view(*v_611)
        v_613 = [int(((v_597.size(0) * 2) * 2)), -1, int(v_597.size(2))]
        v_614 = torch.permute(input=v_604, dims=(0,1,3,2,4,5))
        v_615 = torch.permute(input=v_608, dims=(0,1,3,2,4,5))
        v_616 = v_615.reshape(*v_613)
        v_617 = v_614.reshape(*v_613)
        v_618 = torch.permute(input=v_616, dims=(0,2,1))
        v_619 = torch.matmul(input=v_617, other=v_618)
        v_620 = [int(v_597.size(0)), 1, 1]
        v_621 = v_276.repeat(*v_620)
        v_622 = ((v_619 / torch.pow(v_597.size(2), 5.000000e-01)) + v_621)
        v_623 = torch.permute(input=v_612, dims=(0,1,3,2,4,5))
        v_624 = F.softmax(input=v_622, dim=-1)
        v_625 = v_623.reshape(*v_613)
        v_626 = [int(((v_597.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_597.size(2))]
        v_627 = torch.matmul(input=v_624, other=v_625)
        v_628 = v_627.view(*v_626)
        v_629 = [int(((v_628.size(0) // 2) // 2)), 2, 2, int(v_628.size(1)), int(v_628.size(2)), int(v_628.size(3))]
        v_630 = v_628.view(*v_629)
        v_631 = [int(((v_628.size(0) // 2) // 2)), int((v_628.size(1) * 2)), int((v_628.size(2) * 2)), int(v_628.size(3))]
        v_632 = torch.permute(input=v_630, dims=(0,1,3,2,4,5))
        v_633 = [int(v_597.size(0)), -1, int(v_597.size(2))]
        v_634 = v_632.reshape(*v_631)
        v_635 = torch.roll(input=v_634, shifts=v_150, dims=(1,2))
        v_636 = v_635.view(*v_633)
        v_637 = self.transformer_layers_5_self_attn_merge(v_636)
        v_638 = self.transformer_layers_5_self_attn_norm1(v_637)
        v_639 = (v_594 + v_638)
        v_640 = self.transformer_layers_5_cross_attn_ffn_q_proj(v_639)
        v_641 = torch.cat((v_596, v_595), dim=0)
        v_642 = self.transformer_layers_5_cross_attn_ffn_k_proj(v_641)
        v_643 = self.transformer_layers_5_cross_attn_ffn_v_proj(v_641)
        v_644 = [int(v_640.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_640.size(2))]
        v_645 = v_640.view(*v_644)
        v_646 = torch.roll(input=v_645, shifts=v_252, dims=(1,2))
        v_647 = [int(v_646.size(0)), 2, int((v_646.size(1) // 2)), 2, int((v_646.size(2) // 2)), int(v_646.size(3))]
        v_648 = v_646.view(*v_647)
        v_649 = v_642.view(*v_644)
        v_650 = torch.roll(input=v_649, shifts=v_252, dims=(1,2))
        v_651 = [int(v_650.size(0)), 2, int((v_650.size(1) // 2)), 2, int((v_650.size(2) // 2)), int(v_650.size(3))]
        v_652 = v_650.view(*v_651)
        v_653 = v_643.view(*v_644)
        v_654 = torch.roll(input=v_653, shifts=v_252, dims=(1,2))
        v_655 = [int(v_654.size(0)), 2, int((v_654.size(1) // 2)), 2, int((v_654.size(2) // 2)), int(v_654.size(3))]
        v_656 = v_654.view(*v_655)
        v_657 = [int(((v_640.size(0) * 2) * 2)), -1, int(v_640.size(2))]
        v_658 = torch.permute(input=v_648, dims=(0,1,3,2,4,5))
        v_659 = torch.permute(input=v_652, dims=(0,1,3,2,4,5))
        v_660 = v_659.reshape(*v_657)
        v_661 = v_658.reshape(*v_657)
        v_662 = torch.permute(input=v_660, dims=(0,2,1))
        v_663 = torch.matmul(input=v_661, other=v_662)
        v_664 = [int(v_640.size(0)), 1, 1]
        v_665 = v_276.repeat(*v_664)
        v_666 = ((v_663 / torch.pow(v_640.size(2), 5.000000e-01)) + v_665)
        v_667 = torch.permute(input=v_656, dims=(0,1,3,2,4,5))
        v_668 = F.softmax(input=v_666, dim=-1)
        v_669 = v_667.reshape(*v_657)
        v_670 = [int(((v_640.size(0) * 2) * 2)), int((v_123.size(2) // 2)), int((v_123.size(3) // 2)), int(v_640.size(2))]
        v_671 = torch.matmul(input=v_668, other=v_669)
        v_672 = v_671.view(*v_670)
        v_673 = [int(((v_672.size(0) // 2) // 2)), 2, 2, int(v_672.size(1)), int(v_672.size(2)), int(v_672.size(3))]
        v_674 = v_672.view(*v_673)
        v_675 = [int(((v_672.size(0) // 2) // 2)), int((v_672.size(1) * 2)), int((v_672.size(2) * 2)), int(v_672.size(3))]
        v_676 = torch.permute(input=v_674, dims=(0,1,3,2,4,5))
        v_677 = [int(v_640.size(0)), -1, int(v_640.size(2))]
        v_678 = v_676.reshape(*v_675)
        v_679 = torch.roll(input=v_678, shifts=v_150, dims=(1,2))
        v_680 = v_679.view(*v_677)
        v_681 = self.transformer_layers_5_cross_attn_ffn_merge(v_680)
        v_682 = self.transformer_layers_5_cross_attn_ffn_norm1(v_681)
        v_683 = torch.cat((v_639, v_682), dim=-1)
        v_684 = self.transformer_layers_5_cross_attn_ffn_mlp_0(v_683)
        v_685 = self.transformer_layers_5_cross_attn_ffn_mlp_1(v_684)
        v_686 = self.transformer_layers_5_cross_attn_ffn_mlp_2(v_685)
        v_687 = self.transformer_layers_5_cross_attn_ffn_norm2(v_686)
        v_688 = (v_639 + v_687)
        v_689, v_690 = torch.chunk(input=v_688, chunks=2, dim=0)
        v_691 = [int(v_123.size(0)), int(v_123.size(2)), int(v_123.size(3)), int(v_123.size(1))]
        v_692 = v_689.view(*v_691)
        v_693 = v_690.view(*v_691)
        v_694 = torch.permute(input=v_693, dims=(0,3,1,2))
        v_695 = torch.permute(input=v_692, dims=(0,3,1,2))
        v_696 = v_695.contiguous(memory_format=torch.contiguous_format)
        v_697 = v_696.size(2)
        v_698 = v_696.size(3)
        v_699 = [int(v_696.size(0)), int(v_696.size(1)), -1]
        v_700 = v_696.view(*v_699)
        v_701 = [int(v_696.size(0)), int(v_696.size(2)), int(v_696.size(3)), int(v_696.size(2)), int(v_696.size(3))]
        v_702 = torch.permute(input=v_700, dims=(0,2,1))
        v_703 = v_694.reshape(*v_699)
        v_704 = torch.matmul(input=v_702, other=v_703)
        v_705 = v_704.view(*v_701)
        v_706 = (v_705 / torch.pow(v_696.size(1), 5.000000e-01))
        v_707 = torch.arange(end=v_698)
        v_708 = torch.arange(end=v_697)
        v_709 = [int(v_696.size(0)), 1, int(v_696.size(2)), int(v_696.size(3))]
        v_710 = v_708.view(1, 1, -1, 1)
        v_711 = v_710.expand(*v_709)
        v_712 = v_707.view(1, 1, 1, -1)
        v_713 = v_712.expand(*v_709)
        v_714 = torch.cat((v_713, v_711), dim=1)
        v_715 = [int(v_696.size(0)), 2, -1]
        v_716 = v_714.view(*v_715)
        v_717 = [int(v_696.size(0)), int((v_696.size(2) * v_696.size(3))), int((v_696.size(2) * v_696.size(3)))]
        v_718 = v_706.view(*v_717)
        v_719 = F.softmax(input=v_718, dim=-1)
        v_720 = [int(v_696.size(0)), int(v_696.size(2)), int(v_696.size(3)), 2]
        v_721 = torch.permute(input=v_716, dims=(0,2,1))
        v_722 = torch.matmul(input=v_719, other=v_721)
        v_723 = v_722.view(*v_720)
        v_724 = torch.permute(input=v_723, dims=(0,3,1,2))
        v_725 = (v_724 - v_714)
        v_726 = [int(v_696.size(0)), int(v_696.size(1)), int((v_696.size(2) * v_696.size(3)))]
        v_727 = v_696.view(*v_726)
        v_728 = torch.permute(input=v_727, dims=(0,2,1))
        v_729 = self.feature_flow_attn_q_proj(v_728)
        v_730 = self.feature_flow_attn_k_proj(v_729)
        v_731 = [int(v_696.size(0)), int(v_725.size(1)), int((v_696.size(2) * v_696.size(3)))]
        v_732 = v_725.view(*v_731)
        v_733 = torch.permute(input=v_730, dims=(0,2,1))
        v_734 = torch.matmul(input=v_729, other=v_733)
        v_735 = (v_734 / torch.pow(v_696.size(1), 5.000000e-01))
        v_736 = F.softmax(input=v_735, dim=-1)
        v_737 = torch.permute(input=v_732, dims=(0,2,1))
        v_738 = [int(v_696.size(0)), int(v_696.size(2)), int(v_696.size(3)), int(v_737.size(-1))]
        v_739 = torch.matmul(input=v_736, other=v_737)
        v_740 = v_739.view(*v_738)
        v_741 = torch.permute(input=v_740, dims=(0,3,1,2))
        v_742 = F.upsample(input=v_741, align_corners=True, mode='bilinear', scale_factor=(2.000000,2.000000))
        v_743 = (v_742 * 2)
        v_744 = v_74.size(2)
        v_745 = v_74.size(3)
        v_746 = torch.arange(end=v_745)
        v_747 = torch.arange(end=v_744)
        v_748 = [int(v_74.size(0)), 1, int(v_74.size(2)), int(v_74.size(3))]
        v_749 = v_747.view(1, 1, -1, 1)
        v_750 = v_749.expand(*v_748)
        v_751 = v_746.view(1, 1, 1, -1)
        v_752 = v_751.expand(*v_748)
        v_753 = torch.cat((v_752, v_750), dim=1)
        v_754 = (v_753 + v_743)
        v_755 = v_754.select(dim=1, index=0)
        v_756 = (((v_755 * 2) / (v_754.size(3) - 1)) - 1)
        v_757 = v_754.select(dim=1, index=1)
        v_758 = (((v_757 * 2) / (v_754.size(2) - 1)) - 1)
        v_759 = [int(v_73.size(0)), int(v_73.size(1)), 8, int((v_73.size(2) // 8)), 8, int((v_73.size(3) // 8))]
        v_760 = v_73.view(*v_759)
        v_761 = [int(((v_73.size(0) * 8) * 8)), int(v_73.size(1)), int((v_73.size(2) // 8)), int((v_73.size(3) // 8))]
        v_762 = torch.stack((v_756, v_758), dim=-1)
        v_763 = F.grid_sample(input=v_74, grid=v_762, align_corners=True, mode='bilinear', padding_mode='zeros')
        v_764 = [int(v_763.size(0)), int(v_763.size(1)), 8, int((v_763.size(2) // 8)), 8, int((v_763.size(3) // 8))]
        v_765 = v_763.view(*v_764)
        v_766 = [int(((v_763.size(0) * 8) * 8)), int(v_763.size(1)), int((v_763.size(2) // 8)), int((v_763.size(3) // 8))]
        v_767 = torch.permute(input=v_760, dims=(0,2,4,1,3,5))
        v_768 = v_767.reshape(*v_761)
        v_769 = [int(v_768.size(0)), int(v_768.size(2)), int(v_768.size(3))]
        v_770 = torch.ones(size=v_769)
        v_771 = torch.cumsum(input=v_770, dim=1)
        v_772 = v_771[:,-1:]
        v_773 = ((v_771 / (v_772 + 1.000000e-06)) * 6.283185e+00)
        v_774 = torch.cumsum(input=v_770, dim=2)
        v_775 = v_774[:,:,-1:]
        v_776 = ((v_774 / (v_775 + 1.000000e-06)) * 6.283185e+00)
        v_777 = torch.unsqueeze(input=v_776, dim=3)
        v_778 = (v_777 / v_92)
        v_779 = torch.unsqueeze(input=v_773, dim=3)
        v_780 = (v_779 / v_92)
        v_781 = v_778[:,:,:,::2]
        v_782 = torch.sin(v_781)
        v_783 = v_778[:,:,:,1::2]
        v_784 = torch.cos(v_783)
        v_785 = torch.stack((v_782, v_784), dim=4)
        v_786 = v_780[:,:,:,::2]
        v_787 = torch.sin(v_786)
        v_788 = v_780[:,:,:,1::2]
        v_789 = torch.cos(v_788)
        v_790 = torch.stack((v_787, v_789), dim=4)
        v_791 = torch.flatten(input=v_790, end_dim=-1, start_dim=3)
        v_792 = torch.flatten(input=v_785, end_dim=-1, start_dim=3)
        v_793 = torch.cat((v_791, v_792), dim=3)
        v_794 = torch.permute(input=v_793, dims=(0,3,1,2))
        v_795 = (v_768 + v_794)
        v_796 = torch.permute(input=v_765, dims=(0,2,4,1,3,5))
        v_797 = v_796.reshape(*v_766)
        v_798 = (v_797 + v_794)
        v_799 = [int(((v_795.size(0) // 8) // 8)), 8, 8, int(v_795.size(1)), int(v_795.size(2)), int(v_795.size(3))]
        v_800 = v_795.view(*v_799)
        v_801 = [int(((v_795.size(0) // 8) // 8)), int(v_795.size(1)), int((v_795.size(2) * 8)), int((v_795.size(3) * 8))]
        v_802 = torch.permute(input=v_800, dims=(0,3,1,4,2,5))
        v_803 = [int(((v_798.size(0) // 8) // 8)), 8, 8, int(v_798.size(1)), int(v_798.size(2)), int(v_798.size(3))]
        v_804 = v_798.view(*v_803)
        v_805 = [int(((v_798.size(0) // 8) // 8)), int(v_798.size(1)), int((v_798.size(2) * 8)), int((v_798.size(3) * 8))]
        v_806 = torch.permute(input=v_804, dims=(0,3,1,4,2,5))
        v_807 = v_802.reshape(*v_801)
        v_808 = torch.flatten(input=v_807, end_dim=-1, start_dim=-2)
        v_809 = v_806.reshape(*v_805)
        v_810 = torch.flatten(input=v_809, end_dim=-1, start_dim=-2)
        v_811 = [int((v_807.size(2) - (v_807.size(2) // 8))), int((v_807.size(3) - (v_807.size(3) // 8)))]
        v_812 = torch.ones(size=v_811)
        v_813 = [int((v_807.size(2) - (v_807.size(2) // 8))), int(((v_807.size(3) // 8) - ((v_807.size(3) // 8) // 2)))]
        v_814 = torch.ones(size=v_813)
        v_815 = (v_814 * 2)
        v_816 = [int((v_807.size(2) - (v_807.size(2) // 8))), int(((v_807.size(3) // 8) // 2))]
        v_817 = torch.ones(size=v_816)
        v_818 = (v_817 * 3)
        v_819 = [int(((v_807.size(2) // 8) - ((v_807.size(2) // 8) // 2))), int((v_807.size(3) - (v_807.size(3) // 8)))]
        v_820 = torch.ones(size=v_819)
        v_821 = (v_820 * 4)
        v_822 = [int(((v_807.size(2) // 8) - ((v_807.size(2) // 8) // 2))), int(((v_807.size(3) // 8) - ((v_807.size(3) // 8) // 2)))]
        v_823 = torch.ones(size=v_822)
        v_824 = (v_823 * 5)
        v_825 = [int(((v_807.size(2) // 8) - ((v_807.size(2) // 8) // 2))), int(((v_807.size(3) // 8) // 2))]
        v_826 = torch.ones(size=v_825)
        v_827 = (v_826 * 6)
        v_828 = [int(((v_807.size(2) // 8) // 2)), int((v_807.size(3) - (v_807.size(3) // 8)))]
        v_829 = torch.ones(size=v_828)
        v_830 = (v_829 * 7)
        v_831 = [int(((v_807.size(2) // 8) // 2)), int(((v_807.size(3) // 8) - ((v_807.size(3) // 8) // 2)))]
        v_832 = torch.ones(size=v_831)
        v_833 = (v_832 * 8)
        v_834 = [int(((v_807.size(2) // 8) // 2)), int(((v_807.size(3) // 8) // 2))]
        v_835 = torch.ones(size=v_834)
        v_836 = (v_835 * 9)
        v_837 = torch.cat((v_830, v_833, v_836), dim=1)
        v_838 = torch.cat((v_821, v_824, v_827), dim=1)
        v_839 = torch.cat((v_812, v_815, v_818), dim=1)
        v_840 = torch.cat((v_839, v_838, v_837), dim=0)
        v_841 = torch.unsqueeze(input=v_840, dim=0)
        v_842 = torch.unsqueeze(input=v_841, dim=-1)
        v_843 = [int(v_842.size(0)), int((v_807.size(3) // (v_807.size(3) // 8))), int((v_842.size(1) // (v_807.size(3) // (v_807.size(3) // 8)))), int((v_807.size(3) // (v_807.size(3) // 8))), int((v_842.size(2) // (v_807.size(3) // (v_807.size(3) // 8)))), int(v_842.size(3))]
        v_844 = v_842.view(*v_843)
        v_845 = [-1, int(((v_807.size(2) // 8) * (v_807.size(3) // 8)))]
        v_846 = torch.permute(input=v_844, dims=(0,1,3,2,4,5))
        v_847 = v_846.reshape(*v_845)
        v_848 = torch.unsqueeze(input=v_847, dim=2)
        v_849 = torch.unsqueeze(input=v_847, dim=1)
        v_850 = (v_849 - v_848)
        v_851 = torch.permute(input=v_810, dims=(0,2,1))
        v_852 = torch.permute(input=v_808, dims=(0,2,1))
        v_853 = torch.cat((v_852, v_851), dim=0)
        v_854 = self.pnnx_unique_12(v_853)
        v_855 = self.pnnx_unique_13(v_853)
        v_856 = self.pnnx_unique_14(v_853)
        v_857 = [int(v_854.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_854.size(2))]
        v_858 = v_854.view(*v_857)
        v_859 = [int(v_858.size(0)), 8, int((v_858.size(1) // 8)), 8, int((v_858.size(2) // 8)), int(v_858.size(3))]
        v_860 = v_858.view(*v_859)
        v_861 = v_855.view(*v_857)
        v_862 = [int(v_861.size(0)), 8, int((v_861.size(1) // 8)), 8, int((v_861.size(2) // 8)), int(v_861.size(3))]
        v_863 = v_861.view(*v_862)
        v_864 = v_856.view(*v_857)
        v_865 = [int(v_864.size(0)), 8, int((v_864.size(1) // 8)), 8, int((v_864.size(2) // 8)), int(v_864.size(3))]
        v_866 = v_864.view(*v_865)
        v_867 = [int(((v_854.size(0) * 8) * 8)), -1, int(v_854.size(2))]
        v_868 = torch.permute(input=v_860, dims=(0,1,3,2,4,5))
        v_869 = torch.permute(input=v_863, dims=(0,1,3,2,4,5))
        v_870 = v_869.reshape(*v_867)
        v_871 = v_868.reshape(*v_867)
        v_872 = torch.permute(input=v_870, dims=(0,2,1))
        v_873 = torch.matmul(input=v_871, other=v_872)
        v_874 = (v_873 / torch.pow(v_854.size(2), 5.000000e-01))
        v_875 = torch.permute(input=v_866, dims=(0,1,3,2,4,5))
        v_876 = F.softmax(input=v_874, dim=-1)
        v_877 = v_875.reshape(*v_867)
        v_878 = [int(((v_854.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_854.size(2))]
        v_879 = torch.matmul(input=v_876, other=v_877)
        v_880 = v_879.view(*v_878)
        v_881 = [int(((v_880.size(0) // 8) // 8)), 8, 8, int(v_880.size(1)), int(v_880.size(2)), int(v_880.size(3))]
        v_882 = v_880.view(*v_881)
        v_883 = torch.permute(input=v_882, dims=(0,1,3,2,4,5))
        v_884 = [int(v_854.size(0)), -1, int(v_854.size(2))]
        v_885 = v_883.reshape(*v_884)
        v_886 = self.pnnx_unique_15(v_885)
        v_887 = self.pnnx_unique_16(v_886)
        v_888 = (v_853 + v_887)
        v_889 = self.pnnx_unique_17(v_888)
        v_890 = torch.cat((v_851, v_852), dim=0)
        v_891 = self.pnnx_unique_18(v_890)
        v_892 = self.pnnx_unique_19(v_890)
        v_893 = [int(v_889.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_889.size(2))]
        v_894 = v_889.view(*v_893)
        v_895 = [int(v_894.size(0)), 8, int((v_894.size(1) // 8)), 8, int((v_894.size(2) // 8)), int(v_894.size(3))]
        v_896 = v_894.view(*v_895)
        v_897 = v_891.view(*v_893)
        v_898 = [int(v_897.size(0)), 8, int((v_897.size(1) // 8)), 8, int((v_897.size(2) // 8)), int(v_897.size(3))]
        v_899 = v_897.view(*v_898)
        v_900 = v_892.view(*v_893)
        v_901 = [int(v_900.size(0)), 8, int((v_900.size(1) // 8)), 8, int((v_900.size(2) // 8)), int(v_900.size(3))]
        v_902 = v_900.view(*v_901)
        v_903 = [int(((v_889.size(0) * 8) * 8)), -1, int(v_889.size(2))]
        v_904 = torch.permute(input=v_896, dims=(0,1,3,2,4,5))
        v_905 = torch.permute(input=v_899, dims=(0,1,3,2,4,5))
        v_906 = v_905.reshape(*v_903)
        v_907 = v_904.reshape(*v_903)
        v_908 = torch.permute(input=v_906, dims=(0,2,1))
        v_909 = torch.matmul(input=v_907, other=v_908)
        v_910 = (v_909 / torch.pow(v_889.size(2), 5.000000e-01))
        v_911 = torch.permute(input=v_902, dims=(0,1,3,2,4,5))
        v_912 = F.softmax(input=v_910, dim=-1)
        v_913 = v_911.reshape(*v_903)
        v_914 = [int(((v_889.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_889.size(2))]
        v_915 = torch.matmul(input=v_912, other=v_913)
        v_916 = v_915.view(*v_914)
        v_917 = [int(((v_916.size(0) // 8) // 8)), 8, 8, int(v_916.size(1)), int(v_916.size(2)), int(v_916.size(3))]
        v_918 = v_916.view(*v_917)
        v_919 = torch.permute(input=v_918, dims=(0,1,3,2,4,5))
        v_920 = [int(v_889.size(0)), -1, int(v_889.size(2))]
        v_921 = v_919.reshape(*v_920)
        v_922 = self.pnnx_unique_20(v_921)
        v_923 = self.pnnx_unique_21(v_922)
        v_924 = torch.cat((v_888, v_923), dim=-1)
        v_925 = self.pnnx_unique_22(v_924)
        v_926 = self.pnnx_unique_23(v_925)
        v_927 = self.pnnx_unique_24(v_926)
        v_928 = self.pnnx_unique_25(v_927)
        v_929 = (v_888 + v_928)
        v_930, v_931 = torch.chunk(input=v_929, chunks=2, dim=0)
        v_932 = self.pnnx_unique_26(v_929)
        v_933 = self.pnnx_unique_27(v_929)
        v_934 = self.pnnx_unique_28(v_929)
        v_935 = [int(v_932.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_932.size(2))]
        v_936 = [int(torch.neg(((v_807.size(2) // 8) // 2))), int(torch.neg(((v_807.size(3) // 8) // 2)))]
        v_937 = v_932.view(*v_935)
        v_938 = torch.roll(input=v_937, shifts=v_936, dims=(1,2))
        v_939 = [int(v_938.size(0)), 8, int((v_938.size(1) // 8)), 8, int((v_938.size(2) // 8)), int(v_938.size(3))]
        v_940 = v_938.view(*v_939)
        v_941 = v_933.view(*v_935)
        v_942 = torch.roll(input=v_941, shifts=v_936, dims=(1,2))
        v_943 = [int(v_942.size(0)), 8, int((v_942.size(1) // 8)), 8, int((v_942.size(2) // 8)), int(v_942.size(3))]
        v_944 = v_942.view(*v_943)
        v_945 = v_934.view(*v_935)
        v_946 = torch.roll(input=v_945, shifts=v_936, dims=(1,2))
        v_947 = [int(v_946.size(0)), 8, int((v_946.size(1) // 8)), 8, int((v_946.size(2) // 8)), int(v_946.size(3))]
        v_948 = v_946.view(*v_947)
        v_949 = [int(((v_932.size(0) * 8) * 8)), -1, int(v_932.size(2))]
        v_950 = torch.permute(input=v_940, dims=(0,1,3,2,4,5))
        v_951 = torch.permute(input=v_944, dims=(0,1,3,2,4,5))
        v_952 = v_951.reshape(*v_949)
        v_953 = v_950.reshape(*v_949)
        v_954 = torch.permute(input=v_952, dims=(0,2,1))
        v_955 = torch.matmul(input=v_953, other=v_954)
        v_956 = [int(v_932.size(0)), 1, 1]
        v_957 = torch.ne(input=v_850, other=0)
        v_958 = v_850.masked_fill(v_957, value=-100.000000)
        v_959 = torch.eq(input=v_850, other=0)
        v_960 = v_958.masked_fill(v_959, value=0.000000)
        v_961 = v_960.repeat(*v_956)
        v_962 = ((v_955 / torch.pow(v_932.size(2), 5.000000e-01)) + v_961)
        v_963 = torch.permute(input=v_948, dims=(0,1,3,2,4,5))
        v_964 = F.softmax(input=v_962, dim=-1)
        v_965 = v_963.reshape(*v_949)
        v_966 = [int(((v_932.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_932.size(2))]
        v_967 = torch.matmul(input=v_964, other=v_965)
        v_968 = v_967.view(*v_966)
        v_969 = [int(((v_968.size(0) // 8) // 8)), 8, 8, int(v_968.size(1)), int(v_968.size(2)), int(v_968.size(3))]
        v_970 = v_968.view(*v_969)
        v_971 = [int(((v_968.size(0) // 8) // 8)), int((v_968.size(1) * 8)), int((v_968.size(2) * 8)), int(v_968.size(3))]
        v_972 = torch.permute(input=v_970, dims=(0,1,3,2,4,5))
        v_973 = [int(v_932.size(0)), -1, int(v_932.size(2))]
        v_974 = v_972.reshape(*v_971)
        v_975 = torch.roll(input=v_974, shifts=v_834, dims=(1,2))
        v_976 = v_975.view(*v_973)
        v_977 = self.pnnx_unique_29(v_976)
        v_978 = self.pnnx_unique_30(v_977)
        v_979 = (v_929 + v_978)
        v_980 = self.pnnx_unique_31(v_979)
        v_981 = torch.cat((v_931, v_930), dim=0)
        v_982 = self.pnnx_unique_32(v_981)
        v_983 = self.pnnx_unique_33(v_981)
        v_984 = [int(v_980.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_980.size(2))]
        v_985 = v_980.view(*v_984)
        v_986 = torch.roll(input=v_985, shifts=v_936, dims=(1,2))
        v_987 = [int(v_986.size(0)), 8, int((v_986.size(1) // 8)), 8, int((v_986.size(2) // 8)), int(v_986.size(3))]
        v_988 = v_986.view(*v_987)
        v_989 = v_982.view(*v_984)
        v_990 = torch.roll(input=v_989, shifts=v_936, dims=(1,2))
        v_991 = [int(v_990.size(0)), 8, int((v_990.size(1) // 8)), 8, int((v_990.size(2) // 8)), int(v_990.size(3))]
        v_992 = v_990.view(*v_991)
        v_993 = v_983.view(*v_984)
        v_994 = torch.roll(input=v_993, shifts=v_936, dims=(1,2))
        v_995 = [int(v_994.size(0)), 8, int((v_994.size(1) // 8)), 8, int((v_994.size(2) // 8)), int(v_994.size(3))]
        v_996 = v_994.view(*v_995)
        v_997 = [int(((v_980.size(0) * 8) * 8)), -1, int(v_980.size(2))]
        v_998 = torch.permute(input=v_988, dims=(0,1,3,2,4,5))
        v_999 = torch.permute(input=v_992, dims=(0,1,3,2,4,5))
        v_1000 = v_999.reshape(*v_997)
        v_1001 = v_998.reshape(*v_997)
        v_1002 = torch.permute(input=v_1000, dims=(0,2,1))
        v_1003 = torch.matmul(input=v_1001, other=v_1002)
        v_1004 = [int(v_980.size(0)), 1, 1]
        v_1005 = v_960.repeat(*v_1004)
        v_1006 = ((v_1003 / torch.pow(v_980.size(2), 5.000000e-01)) + v_1005)
        v_1007 = torch.permute(input=v_996, dims=(0,1,3,2,4,5))
        v_1008 = F.softmax(input=v_1006, dim=-1)
        v_1009 = v_1007.reshape(*v_997)
        v_1010 = [int(((v_980.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_980.size(2))]
        v_1011 = torch.matmul(input=v_1008, other=v_1009)
        v_1012 = v_1011.view(*v_1010)
        v_1013 = [int(((v_1012.size(0) // 8) // 8)), 8, 8, int(v_1012.size(1)), int(v_1012.size(2)), int(v_1012.size(3))]
        v_1014 = v_1012.view(*v_1013)
        v_1015 = [int(((v_1012.size(0) // 8) // 8)), int((v_1012.size(1) * 8)), int((v_1012.size(2) * 8)), int(v_1012.size(3))]
        v_1016 = torch.permute(input=v_1014, dims=(0,1,3,2,4,5))
        v_1017 = [int(v_980.size(0)), -1, int(v_980.size(2))]
        v_1018 = v_1016.reshape(*v_1015)
        v_1019 = torch.roll(input=v_1018, shifts=v_834, dims=(1,2))
        v_1020 = v_1019.view(*v_1017)
        v_1021 = self.pnnx_unique_34(v_1020)
        v_1022 = self.pnnx_unique_35(v_1021)
        v_1023 = torch.cat((v_979, v_1022), dim=-1)
        v_1024 = self.pnnx_unique_36(v_1023)
        v_1025 = self.pnnx_unique_37(v_1024)
        v_1026 = self.pnnx_unique_38(v_1025)
        v_1027 = self.pnnx_unique_39(v_1026)
        v_1028 = (v_979 + v_1027)
        v_1029, v_1030 = torch.chunk(input=v_1028, chunks=2, dim=0)
        v_1031 = self.pnnx_unique_40(v_1028)
        v_1032 = self.pnnx_unique_41(v_1028)
        v_1033 = self.pnnx_unique_42(v_1028)
        v_1034 = [int(v_1031.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1031.size(2))]
        v_1035 = v_1031.view(*v_1034)
        v_1036 = [int(v_1035.size(0)), 8, int((v_1035.size(1) // 8)), 8, int((v_1035.size(2) // 8)), int(v_1035.size(3))]
        v_1037 = v_1035.view(*v_1036)
        v_1038 = v_1032.view(*v_1034)
        v_1039 = [int(v_1038.size(0)), 8, int((v_1038.size(1) // 8)), 8, int((v_1038.size(2) // 8)), int(v_1038.size(3))]
        v_1040 = v_1038.view(*v_1039)
        v_1041 = v_1033.view(*v_1034)
        v_1042 = [int(v_1041.size(0)), 8, int((v_1041.size(1) // 8)), 8, int((v_1041.size(2) // 8)), int(v_1041.size(3))]
        v_1043 = v_1041.view(*v_1042)
        v_1044 = [int(((v_1031.size(0) * 8) * 8)), -1, int(v_1031.size(2))]
        v_1045 = torch.permute(input=v_1037, dims=(0,1,3,2,4,5))
        v_1046 = torch.permute(input=v_1040, dims=(0,1,3,2,4,5))
        v_1047 = v_1046.reshape(*v_1044)
        v_1048 = v_1045.reshape(*v_1044)
        v_1049 = torch.permute(input=v_1047, dims=(0,2,1))
        v_1050 = torch.matmul(input=v_1048, other=v_1049)
        v_1051 = (v_1050 / torch.pow(v_1031.size(2), 5.000000e-01))
        v_1052 = torch.permute(input=v_1043, dims=(0,1,3,2,4,5))
        v_1053 = F.softmax(input=v_1051, dim=-1)
        v_1054 = v_1052.reshape(*v_1044)
        v_1055 = [int(((v_1031.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1031.size(2))]
        v_1056 = torch.matmul(input=v_1053, other=v_1054)
        v_1057 = v_1056.view(*v_1055)
        v_1058 = [int(((v_1057.size(0) // 8) // 8)), 8, 8, int(v_1057.size(1)), int(v_1057.size(2)), int(v_1057.size(3))]
        v_1059 = v_1057.view(*v_1058)
        v_1060 = torch.permute(input=v_1059, dims=(0,1,3,2,4,5))
        v_1061 = [int(v_1031.size(0)), -1, int(v_1031.size(2))]
        v_1062 = v_1060.reshape(*v_1061)
        v_1063 = self.pnnx_unique_43(v_1062)
        v_1064 = self.pnnx_unique_44(v_1063)
        v_1065 = (v_1028 + v_1064)
        v_1066 = self.pnnx_unique_45(v_1065)
        v_1067 = torch.cat((v_1030, v_1029), dim=0)
        v_1068 = self.pnnx_unique_46(v_1067)
        v_1069 = self.pnnx_unique_47(v_1067)
        v_1070 = [int(v_1066.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1066.size(2))]
        v_1071 = v_1066.view(*v_1070)
        v_1072 = [int(v_1071.size(0)), 8, int((v_1071.size(1) // 8)), 8, int((v_1071.size(2) // 8)), int(v_1071.size(3))]
        v_1073 = v_1071.view(*v_1072)
        v_1074 = v_1068.view(*v_1070)
        v_1075 = [int(v_1074.size(0)), 8, int((v_1074.size(1) // 8)), 8, int((v_1074.size(2) // 8)), int(v_1074.size(3))]
        v_1076 = v_1074.view(*v_1075)
        v_1077 = v_1069.view(*v_1070)
        v_1078 = [int(v_1077.size(0)), 8, int((v_1077.size(1) // 8)), 8, int((v_1077.size(2) // 8)), int(v_1077.size(3))]
        v_1079 = v_1077.view(*v_1078)
        v_1080 = [int(((v_1066.size(0) * 8) * 8)), -1, int(v_1066.size(2))]
        v_1081 = torch.permute(input=v_1073, dims=(0,1,3,2,4,5))
        v_1082 = torch.permute(input=v_1076, dims=(0,1,3,2,4,5))
        v_1083 = v_1082.reshape(*v_1080)
        v_1084 = v_1081.reshape(*v_1080)
        v_1085 = torch.permute(input=v_1083, dims=(0,2,1))
        v_1086 = torch.matmul(input=v_1084, other=v_1085)
        v_1087 = (v_1086 / torch.pow(v_1066.size(2), 5.000000e-01))
        v_1088 = torch.permute(input=v_1079, dims=(0,1,3,2,4,5))
        v_1089 = F.softmax(input=v_1087, dim=-1)
        v_1090 = v_1088.reshape(*v_1080)
        v_1091 = [int(((v_1066.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1066.size(2))]
        v_1092 = torch.matmul(input=v_1089, other=v_1090)
        v_1093 = v_1092.view(*v_1091)
        v_1094 = [int(((v_1093.size(0) // 8) // 8)), 8, 8, int(v_1093.size(1)), int(v_1093.size(2)), int(v_1093.size(3))]
        v_1095 = v_1093.view(*v_1094)
        v_1096 = torch.permute(input=v_1095, dims=(0,1,3,2,4,5))
        v_1097 = [int(v_1066.size(0)), -1, int(v_1066.size(2))]
        v_1098 = v_1096.reshape(*v_1097)
        v_1099 = self.pnnx_unique_48(v_1098)
        v_1100 = self.pnnx_unique_49(v_1099)
        v_1101 = torch.cat((v_1065, v_1100), dim=-1)
        v_1102 = self.pnnx_unique_50(v_1101)
        v_1103 = self.pnnx_unique_51(v_1102)
        v_1104 = self.pnnx_unique_52(v_1103)
        v_1105 = self.pnnx_unique_53(v_1104)
        v_1106 = (v_1065 + v_1105)
        v_1107, v_1108 = torch.chunk(input=v_1106, chunks=2, dim=0)
        v_1109 = self.pnnx_unique_54(v_1106)
        v_1110 = self.pnnx_unique_55(v_1106)
        v_1111 = self.pnnx_unique_56(v_1106)
        v_1112 = [int(v_1109.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1109.size(2))]
        v_1113 = v_1109.view(*v_1112)
        v_1114 = torch.roll(input=v_1113, shifts=v_936, dims=(1,2))
        v_1115 = [int(v_1114.size(0)), 8, int((v_1114.size(1) // 8)), 8, int((v_1114.size(2) // 8)), int(v_1114.size(3))]
        v_1116 = v_1114.view(*v_1115)
        v_1117 = v_1110.view(*v_1112)
        v_1118 = torch.roll(input=v_1117, shifts=v_936, dims=(1,2))
        v_1119 = [int(v_1118.size(0)), 8, int((v_1118.size(1) // 8)), 8, int((v_1118.size(2) // 8)), int(v_1118.size(3))]
        v_1120 = v_1118.view(*v_1119)
        v_1121 = v_1111.view(*v_1112)
        v_1122 = torch.roll(input=v_1121, shifts=v_936, dims=(1,2))
        v_1123 = [int(v_1122.size(0)), 8, int((v_1122.size(1) // 8)), 8, int((v_1122.size(2) // 8)), int(v_1122.size(3))]
        v_1124 = v_1122.view(*v_1123)
        v_1125 = [int(((v_1109.size(0) * 8) * 8)), -1, int(v_1109.size(2))]
        v_1126 = torch.permute(input=v_1116, dims=(0,1,3,2,4,5))
        v_1127 = torch.permute(input=v_1120, dims=(0,1,3,2,4,5))
        v_1128 = v_1127.reshape(*v_1125)
        v_1129 = v_1126.reshape(*v_1125)
        v_1130 = torch.permute(input=v_1128, dims=(0,2,1))
        v_1131 = torch.matmul(input=v_1129, other=v_1130)
        v_1132 = [int(v_1109.size(0)), 1, 1]
        v_1133 = v_960.repeat(*v_1132)
        v_1134 = ((v_1131 / torch.pow(v_1109.size(2), 5.000000e-01)) + v_1133)
        v_1135 = torch.permute(input=v_1124, dims=(0,1,3,2,4,5))
        v_1136 = F.softmax(input=v_1134, dim=-1)
        v_1137 = v_1135.reshape(*v_1125)
        v_1138 = [int(((v_1109.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1109.size(2))]
        v_1139 = torch.matmul(input=v_1136, other=v_1137)
        v_1140 = v_1139.view(*v_1138)
        v_1141 = [int(((v_1140.size(0) // 8) // 8)), 8, 8, int(v_1140.size(1)), int(v_1140.size(2)), int(v_1140.size(3))]
        v_1142 = v_1140.view(*v_1141)
        v_1143 = [int(((v_1140.size(0) // 8) // 8)), int((v_1140.size(1) * 8)), int((v_1140.size(2) * 8)), int(v_1140.size(3))]
        v_1144 = torch.permute(input=v_1142, dims=(0,1,3,2,4,5))
        v_1145 = [int(v_1109.size(0)), -1, int(v_1109.size(2))]
        v_1146 = v_1144.reshape(*v_1143)
        v_1147 = torch.roll(input=v_1146, shifts=v_834, dims=(1,2))
        v_1148 = v_1147.view(*v_1145)
        v_1149 = self.pnnx_unique_57(v_1148)
        v_1150 = self.pnnx_unique_58(v_1149)
        v_1151 = (v_1106 + v_1150)
        v_1152 = self.pnnx_unique_59(v_1151)
        v_1153 = torch.cat((v_1108, v_1107), dim=0)
        v_1154 = self.pnnx_unique_60(v_1153)
        v_1155 = self.pnnx_unique_61(v_1153)
        v_1156 = [int(v_1152.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1152.size(2))]
        v_1157 = v_1152.view(*v_1156)
        v_1158 = torch.roll(input=v_1157, shifts=v_936, dims=(1,2))
        v_1159 = [int(v_1158.size(0)), 8, int((v_1158.size(1) // 8)), 8, int((v_1158.size(2) // 8)), int(v_1158.size(3))]
        v_1160 = v_1158.view(*v_1159)
        v_1161 = v_1154.view(*v_1156)
        v_1162 = torch.roll(input=v_1161, shifts=v_936, dims=(1,2))
        v_1163 = [int(v_1162.size(0)), 8, int((v_1162.size(1) // 8)), 8, int((v_1162.size(2) // 8)), int(v_1162.size(3))]
        v_1164 = v_1162.view(*v_1163)
        v_1165 = v_1155.view(*v_1156)
        v_1166 = torch.roll(input=v_1165, shifts=v_936, dims=(1,2))
        v_1167 = [int(v_1166.size(0)), 8, int((v_1166.size(1) // 8)), 8, int((v_1166.size(2) // 8)), int(v_1166.size(3))]
        v_1168 = v_1166.view(*v_1167)
        v_1169 = [int(((v_1152.size(0) * 8) * 8)), -1, int(v_1152.size(2))]
        v_1170 = torch.permute(input=v_1160, dims=(0,1,3,2,4,5))
        v_1171 = torch.permute(input=v_1164, dims=(0,1,3,2,4,5))
        v_1172 = v_1171.reshape(*v_1169)
        v_1173 = v_1170.reshape(*v_1169)
        v_1174 = torch.permute(input=v_1172, dims=(0,2,1))
        v_1175 = torch.matmul(input=v_1173, other=v_1174)
        v_1176 = [int(v_1152.size(0)), 1, 1]
        v_1177 = v_960.repeat(*v_1176)
        v_1178 = ((v_1175 / torch.pow(v_1152.size(2), 5.000000e-01)) + v_1177)
        v_1179 = torch.permute(input=v_1168, dims=(0,1,3,2,4,5))
        v_1180 = F.softmax(input=v_1178, dim=-1)
        v_1181 = v_1179.reshape(*v_1169)
        v_1182 = [int(((v_1152.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1152.size(2))]
        v_1183 = torch.matmul(input=v_1180, other=v_1181)
        v_1184 = v_1183.view(*v_1182)
        v_1185 = [int(((v_1184.size(0) // 8) // 8)), 8, 8, int(v_1184.size(1)), int(v_1184.size(2)), int(v_1184.size(3))]
        v_1186 = v_1184.view(*v_1185)
        v_1187 = [int(((v_1184.size(0) // 8) // 8)), int((v_1184.size(1) * 8)), int((v_1184.size(2) * 8)), int(v_1184.size(3))]
        v_1188 = torch.permute(input=v_1186, dims=(0,1,3,2,4,5))
        v_1189 = [int(v_1152.size(0)), -1, int(v_1152.size(2))]
        v_1190 = v_1188.reshape(*v_1187)
        v_1191 = torch.roll(input=v_1190, shifts=v_834, dims=(1,2))
        v_1192 = v_1191.view(*v_1189)
        v_1193 = self.pnnx_unique_62(v_1192)
        v_1194 = self.pnnx_unique_63(v_1193)
        v_1195 = torch.cat((v_1151, v_1194), dim=-1)
        v_1196 = self.pnnx_unique_64(v_1195)
        v_1197 = self.pnnx_unique_65(v_1196)
        v_1198 = self.pnnx_unique_66(v_1197)
        v_1199 = self.pnnx_unique_67(v_1198)
        v_1200 = (v_1151 + v_1199)
        v_1201, v_1202 = torch.chunk(input=v_1200, chunks=2, dim=0)
        v_1203 = self.pnnx_unique_68(v_1200)
        v_1204 = self.pnnx_unique_69(v_1200)
        v_1205 = self.pnnx_unique_70(v_1200)
        v_1206 = [int(v_1203.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1203.size(2))]
        v_1207 = v_1203.view(*v_1206)
        v_1208 = [int(v_1207.size(0)), 8, int((v_1207.size(1) // 8)), 8, int((v_1207.size(2) // 8)), int(v_1207.size(3))]
        v_1209 = v_1207.view(*v_1208)
        v_1210 = v_1204.view(*v_1206)
        v_1211 = [int(v_1210.size(0)), 8, int((v_1210.size(1) // 8)), 8, int((v_1210.size(2) // 8)), int(v_1210.size(3))]
        v_1212 = v_1210.view(*v_1211)
        v_1213 = v_1205.view(*v_1206)
        v_1214 = [int(v_1213.size(0)), 8, int((v_1213.size(1) // 8)), 8, int((v_1213.size(2) // 8)), int(v_1213.size(3))]
        v_1215 = v_1213.view(*v_1214)
        v_1216 = [int(((v_1203.size(0) * 8) * 8)), -1, int(v_1203.size(2))]
        v_1217 = torch.permute(input=v_1209, dims=(0,1,3,2,4,5))
        v_1218 = torch.permute(input=v_1212, dims=(0,1,3,2,4,5))
        v_1219 = v_1218.reshape(*v_1216)
        v_1220 = v_1217.reshape(*v_1216)
        v_1221 = torch.permute(input=v_1219, dims=(0,2,1))
        v_1222 = torch.matmul(input=v_1220, other=v_1221)
        v_1223 = (v_1222 / torch.pow(v_1203.size(2), 5.000000e-01))
        v_1224 = torch.permute(input=v_1215, dims=(0,1,3,2,4,5))
        v_1225 = F.softmax(input=v_1223, dim=-1)
        v_1226 = v_1224.reshape(*v_1216)
        v_1227 = [int(((v_1203.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1203.size(2))]
        v_1228 = torch.matmul(input=v_1225, other=v_1226)
        v_1229 = v_1228.view(*v_1227)
        v_1230 = [int(((v_1229.size(0) // 8) // 8)), 8, 8, int(v_1229.size(1)), int(v_1229.size(2)), int(v_1229.size(3))]
        v_1231 = v_1229.view(*v_1230)
        v_1232 = torch.permute(input=v_1231, dims=(0,1,3,2,4,5))
        v_1233 = [int(v_1203.size(0)), -1, int(v_1203.size(2))]
        v_1234 = v_1232.reshape(*v_1233)
        v_1235 = self.pnnx_unique_71(v_1234)
        v_1236 = self.pnnx_unique_72(v_1235)
        v_1237 = (v_1200 + v_1236)
        v_1238 = self.pnnx_unique_73(v_1237)
        v_1239 = torch.cat((v_1202, v_1201), dim=0)
        v_1240 = self.pnnx_unique_74(v_1239)
        v_1241 = self.pnnx_unique_75(v_1239)
        v_1242 = [int(v_1238.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1238.size(2))]
        v_1243 = v_1238.view(*v_1242)
        v_1244 = [int(v_1243.size(0)), 8, int((v_1243.size(1) // 8)), 8, int((v_1243.size(2) // 8)), int(v_1243.size(3))]
        v_1245 = v_1243.view(*v_1244)
        v_1246 = v_1240.view(*v_1242)
        v_1247 = [int(v_1246.size(0)), 8, int((v_1246.size(1) // 8)), 8, int((v_1246.size(2) // 8)), int(v_1246.size(3))]
        v_1248 = v_1246.view(*v_1247)
        v_1249 = v_1241.view(*v_1242)
        v_1250 = [int(v_1249.size(0)), 8, int((v_1249.size(1) // 8)), 8, int((v_1249.size(2) // 8)), int(v_1249.size(3))]
        v_1251 = v_1249.view(*v_1250)
        v_1252 = [int(((v_1238.size(0) * 8) * 8)), -1, int(v_1238.size(2))]
        v_1253 = torch.permute(input=v_1245, dims=(0,1,3,2,4,5))
        v_1254 = torch.permute(input=v_1248, dims=(0,1,3,2,4,5))
        v_1255 = v_1254.reshape(*v_1252)
        v_1256 = v_1253.reshape(*v_1252)
        v_1257 = torch.permute(input=v_1255, dims=(0,2,1))
        v_1258 = torch.matmul(input=v_1256, other=v_1257)
        v_1259 = (v_1258 / torch.pow(v_1238.size(2), 5.000000e-01))
        v_1260 = torch.permute(input=v_1251, dims=(0,1,3,2,4,5))
        v_1261 = F.softmax(input=v_1259, dim=-1)
        v_1262 = v_1260.reshape(*v_1252)
        v_1263 = [int(((v_1238.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1238.size(2))]
        v_1264 = torch.matmul(input=v_1261, other=v_1262)
        v_1265 = v_1264.view(*v_1263)
        v_1266 = [int(((v_1265.size(0) // 8) // 8)), 8, 8, int(v_1265.size(1)), int(v_1265.size(2)), int(v_1265.size(3))]
        v_1267 = v_1265.view(*v_1266)
        v_1268 = torch.permute(input=v_1267, dims=(0,1,3,2,4,5))
        v_1269 = [int(v_1238.size(0)), -1, int(v_1238.size(2))]
        v_1270 = v_1268.reshape(*v_1269)
        v_1271 = self.pnnx_unique_76(v_1270)
        v_1272 = self.pnnx_unique_77(v_1271)
        v_1273 = torch.cat((v_1237, v_1272), dim=-1)
        v_1274 = self.pnnx_unique_78(v_1273)
        v_1275 = self.pnnx_unique_79(v_1274)
        v_1276 = self.pnnx_unique_80(v_1275)
        v_1277 = self.pnnx_unique_81(v_1276)
        v_1278 = (v_1237 + v_1277)
        v_1279, v_1280 = torch.chunk(input=v_1278, chunks=2, dim=0)
        v_1281 = self.pnnx_unique_82(v_1278)
        v_1282 = self.pnnx_unique_83(v_1278)
        v_1283 = self.pnnx_unique_84(v_1278)
        v_1284 = [int(v_1281.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1281.size(2))]
        v_1285 = v_1281.view(*v_1284)
        v_1286 = torch.roll(input=v_1285, shifts=v_936, dims=(1,2))
        v_1287 = [int(v_1286.size(0)), 8, int((v_1286.size(1) // 8)), 8, int((v_1286.size(2) // 8)), int(v_1286.size(3))]
        v_1288 = v_1286.view(*v_1287)
        v_1289 = v_1282.view(*v_1284)
        v_1290 = torch.roll(input=v_1289, shifts=v_936, dims=(1,2))
        v_1291 = [int(v_1290.size(0)), 8, int((v_1290.size(1) // 8)), 8, int((v_1290.size(2) // 8)), int(v_1290.size(3))]
        v_1292 = v_1290.view(*v_1291)
        v_1293 = v_1283.view(*v_1284)
        v_1294 = torch.roll(input=v_1293, shifts=v_936, dims=(1,2))
        v_1295 = [int(v_1294.size(0)), 8, int((v_1294.size(1) // 8)), 8, int((v_1294.size(2) // 8)), int(v_1294.size(3))]
        v_1296 = v_1294.view(*v_1295)
        v_1297 = [int(((v_1281.size(0) * 8) * 8)), -1, int(v_1281.size(2))]
        v_1298 = torch.permute(input=v_1288, dims=(0,1,3,2,4,5))
        v_1299 = torch.permute(input=v_1292, dims=(0,1,3,2,4,5))
        v_1300 = v_1299.reshape(*v_1297)
        v_1301 = v_1298.reshape(*v_1297)
        v_1302 = torch.permute(input=v_1300, dims=(0,2,1))
        v_1303 = torch.matmul(input=v_1301, other=v_1302)
        v_1304 = [int(v_1281.size(0)), 1, 1]
        v_1305 = v_960.repeat(*v_1304)
        v_1306 = ((v_1303 / torch.pow(v_1281.size(2), 5.000000e-01)) + v_1305)
        v_1307 = torch.permute(input=v_1296, dims=(0,1,3,2,4,5))
        v_1308 = F.softmax(input=v_1306, dim=-1)
        v_1309 = v_1307.reshape(*v_1297)
        v_1310 = [int(((v_1281.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1281.size(2))]
        v_1311 = torch.matmul(input=v_1308, other=v_1309)
        v_1312 = v_1311.view(*v_1310)
        v_1313 = [int(((v_1312.size(0) // 8) // 8)), 8, 8, int(v_1312.size(1)), int(v_1312.size(2)), int(v_1312.size(3))]
        v_1314 = v_1312.view(*v_1313)
        v_1315 = [int(((v_1312.size(0) // 8) // 8)), int((v_1312.size(1) * 8)), int((v_1312.size(2) * 8)), int(v_1312.size(3))]
        v_1316 = torch.permute(input=v_1314, dims=(0,1,3,2,4,5))
        v_1317 = [int(v_1281.size(0)), -1, int(v_1281.size(2))]
        v_1318 = v_1316.reshape(*v_1315)
        v_1319 = torch.roll(input=v_1318, shifts=v_834, dims=(1,2))
        v_1320 = v_1319.view(*v_1317)
        v_1321 = self.pnnx_unique_85(v_1320)
        v_1322 = self.pnnx_unique_86(v_1321)
        v_1323 = (v_1278 + v_1322)
        v_1324 = self.pnnx_unique_87(v_1323)
        v_1325 = torch.cat((v_1280, v_1279), dim=0)
        v_1326 = self.pnnx_unique_88(v_1325)
        v_1327 = self.pnnx_unique_89(v_1325)
        v_1328 = [int(v_1324.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_1324.size(2))]
        v_1329 = v_1324.view(*v_1328)
        v_1330 = torch.roll(input=v_1329, shifts=v_936, dims=(1,2))
        v_1331 = [int(v_1330.size(0)), 8, int((v_1330.size(1) // 8)), 8, int((v_1330.size(2) // 8)), int(v_1330.size(3))]
        v_1332 = v_1330.view(*v_1331)
        v_1333 = v_1326.view(*v_1328)
        v_1334 = torch.roll(input=v_1333, shifts=v_936, dims=(1,2))
        v_1335 = [int(v_1334.size(0)), 8, int((v_1334.size(1) // 8)), 8, int((v_1334.size(2) // 8)), int(v_1334.size(3))]
        v_1336 = v_1334.view(*v_1335)
        v_1337 = v_1327.view(*v_1328)
        v_1338 = torch.roll(input=v_1337, shifts=v_936, dims=(1,2))
        v_1339 = [int(v_1338.size(0)), 8, int((v_1338.size(1) // 8)), 8, int((v_1338.size(2) // 8)), int(v_1338.size(3))]
        v_1340 = v_1338.view(*v_1339)
        v_1341 = [int(((v_1324.size(0) * 8) * 8)), -1, int(v_1324.size(2))]
        v_1342 = torch.permute(input=v_1332, dims=(0,1,3,2,4,5))
        v_1343 = torch.permute(input=v_1336, dims=(0,1,3,2,4,5))
        v_1344 = v_1343.reshape(*v_1341)
        v_1345 = v_1342.reshape(*v_1341)
        v_1346 = torch.permute(input=v_1344, dims=(0,2,1))
        v_1347 = torch.matmul(input=v_1345, other=v_1346)
        v_1348 = [int(v_1324.size(0)), 1, 1]
        v_1349 = v_960.repeat(*v_1348)
        v_1350 = ((v_1347 / torch.pow(v_1324.size(2), 5.000000e-01)) + v_1349)
        v_1351 = torch.permute(input=v_1340, dims=(0,1,3,2,4,5))
        v_1352 = F.softmax(input=v_1350, dim=-1)
        v_1353 = v_1351.reshape(*v_1341)
        v_1354 = [int(((v_1324.size(0) * 8) * 8)), int((v_807.size(2) // 8)), int((v_807.size(3) // 8)), int(v_1324.size(2))]
        v_1355 = torch.matmul(input=v_1352, other=v_1353)
        v_1356 = v_1355.view(*v_1354)
        v_1357 = [int(((v_1356.size(0) // 8) // 8)), 8, 8, int(v_1356.size(1)), int(v_1356.size(2)), int(v_1356.size(3))]
        v_1358 = v_1356.view(*v_1357)
        v_1359 = [int(((v_1356.size(0) // 8) // 8)), int((v_1356.size(1) * 8)), int((v_1356.size(2) * 8)), int(v_1356.size(3))]
        v_1360 = torch.permute(input=v_1358, dims=(0,1,3,2,4,5))
        v_1361 = [int(v_1324.size(0)), -1, int(v_1324.size(2))]
        v_1362 = v_1360.reshape(*v_1359)
        v_1363 = torch.roll(input=v_1362, shifts=v_834, dims=(1,2))
        v_1364 = v_1363.view(*v_1361)
        v_1365 = self.pnnx_unique_90(v_1364)
        v_1366 = self.pnnx_unique_91(v_1365)
        v_1367 = torch.cat((v_1323, v_1366), dim=-1)
        v_1368 = self.pnnx_unique_92(v_1367)
        v_1369 = self.pnnx_unique_93(v_1368)
        v_1370 = self.pnnx_unique_94(v_1369)
        v_1371 = self.pnnx_unique_95(v_1370)
        v_1372 = (v_1323 + v_1371)
        v_1373, v_1374 = torch.chunk(input=v_1372, chunks=2, dim=0)
        v_1375 = [int(v_807.size(0)), int(v_807.size(2)), int(v_807.size(3)), int(v_807.size(1))]
        v_1376 = v_1373.view(*v_1375)
        v_1377 = v_1374.view(*v_1375)
        v_1378 = torch.permute(input=v_1377, dims=(0,3,1,2))
        v_1379 = v_1378.contiguous(memory_format=torch.contiguous_format)
        v_1380 = torch.permute(input=v_1376, dims=(0,3,1,2))
        v_1381 = v_1380.contiguous(memory_format=torch.contiguous_format)
        v_1382 = v_1381.size(2)
        v_1383 = v_1381.size(3)
        v_1384 = torch.arange(end=v_1383)
        v_1385 = torch.arange(end=v_1382)
        v_1386 = [int(v_1381.size(0)), 1, int(v_1381.size(2)), int(v_1381.size(3))]
        v_1387 = v_1385.view(1, 1, -1, 1)
        v_1388 = v_1387.expand(*v_1386)
        v_1389 = v_1384.view(1, 1, 1, -1)
        v_1390 = v_1389.expand(*v_1386)
        v_1391 = torch.cat((v_1390, v_1388), dim=1)
        v_1392 = [int(v_1381.size(0)), 2, -1]
        v_1393 = v_1391.view(*v_1392)
        v_1394 = torch.arange(end=5, start=-4)
        v_1395 = v_1394.view(-1, 1)
        v_1396 = v_1395.expand(-1, 9)
        v_1397 = v_1394.view(1, -1)
        v_1398 = v_1397.expand(9, -1)
        v_1399 = torch.stack((v_1398, v_1396), dim=-1)
        v_1400 = [int(v_1381.size(0)), 1, 1, 1]
        v_1401 = torch.permute(input=v_1393, dims=(0,2,1))
        v_1402 = v_1399.reshape(-1, 2)
        v_1403 = v_1402.repeat(*v_1400)
        v_1404 = torch.unsqueeze(input=v_1401, dim=-2)
        v_1405 = (v_1404 + v_1403)
        v_1406 = ((v_1405 - v_2) / v_2)
        v_1407 = F.grid_sample(input=v_1379, grid=v_1406, align_corners=True, mode='bilinear', padding_mode='zeros')
        v_1408 = [int(v_1381.size(0)), int((v_1381.size(2) * v_1381.size(3))), 1, int(v_1381.size(1))]
        v_1409 = torch.permute(input=v_1381, dims=(0,2,3,1))
        v_1410 = v_1409.view(*v_1408)
        v_1411 = [int(v_1381.size(0)), int((v_1381.size(2) * v_1381.size(3))), -1]
        v_1412 = torch.permute(input=v_1407, dims=(0,2,1,3))
        v_1413 = torch.matmul(input=v_1410, other=v_1412)
        v_1414 = v_1413.view(*v_1411)
        v_1415 = (v_1414 / torch.pow(v_1381.size(1), 5.000000e-01))
        v_1416 = F.softmax(input=v_1415, dim=-1)
        v_1417 = torch.unsqueeze(input=v_1416, dim=-2)
        v_1418 = torch.matmul(input=v_1417, other=v_1405)
        v_1419 = [int(v_1381.size(0)), int(v_1381.size(2)), int(v_1381.size(3)), 2]
        v_1420 = v_1418.reshape(*v_1419)
        v_1421 = torch.permute(input=v_1420, dims=(0,3,1,2))
        v_1422 = (v_743 + (v_1421 - v_1391))
        v_1423 = [int(v_1381.size(0)), int(v_1381.size(1)), -1]
        v_1424 = v_1381.view(*v_1423)
        v_1425 = torch.permute(input=v_1424, dims=(0,2,1))
        v_1426 = self.pnnx_unique_96(v_1425)
        v_1427 = [int(((v_1381.size(0) * v_1381.size(2)) * v_1381.size(3))), 1, int(v_1381.size(1))]
        v_1428 = self.pnnx_unique_97(v_1425)
        v_1429 = [int(v_1381.size(0)), int(v_1381.size(1)), int(v_1381.size(2)), int(v_1381.size(3))]
        v_1430 = [int(v_1381.size(0)), int(v_1381.size(1)), 9, int(v_1381.size(2)), int(v_1381.size(3))]
        v_1431 = torch.permute(input=v_1428, dims=(0,2,1))
        v_1432 = v_1431.reshape(*v_1429)
        v_1433 = F.unfold(input=v_1432, dilation=(1,1), kernel_size=(3,3), padding=(1,1), stride=(1,1))
        v_1434 = v_1433.view(*v_1430)
        v_1435 = [int(((v_1381.size(0) * v_1381.size(2)) * v_1381.size(3))), int(v_1381.size(1)), 9]
        v_1436 = [int(v_1381.size(0)), 2, 9, int(v_1381.size(2)), int(v_1381.size(3))]
        v_1437 = F.unfold(input=v_1422, dilation=(1,1), kernel_size=(3,3), padding=(1,1), stride=(1,1))
        v_1438 = v_1437.view(*v_1436)
        v_1439 = torch.permute(input=v_1434, dims=(0,3,4,1,2))
        v_1440 = v_1439.reshape(*v_1435)
        v_1441 = v_1426.reshape(*v_1427)
        v_1442 = torch.matmul(input=v_1441, other=v_1440)
        v_1443 = (v_1442 / torch.pow(v_1381.size(1), 5.000000e-01))
        v_1444 = F.softmax(input=v_1443, dim=-1)
        v_1445 = torch.permute(input=v_1438, dims=(0,3,4,2,1))
        v_1446 = v_1445.reshape(-1, 9, 2)
        v_1447 = torch.matmul(input=v_1444, other=v_1446)
        v_1448 = v_1447.view(*v_1419)
        v_1449 = torch.permute(input=v_1448, dims=(0,3,1,2))
        v_1450 = v_1449.contiguous(memory_format=torch.contiguous_format)
        v_1451 = torch.cat((v_1450, v_1381), dim=1)
        v_1452 = self.upsampler_0(v_1451)
        v_1453 = self.upsampler_1(v_1452)
        v_1454 = self.upsampler_2(v_1453)
        v_1455 = [int(v_1450.size(0)), 1, 9, 4, 4, int(v_1450.size(2)), int(v_1450.size(3))]
        v_1456 = (v_1450 * 4)
        v_1457 = [int(v_1450.size(0)), int(v_1450.size(1)), 9, 1, 1, int(v_1450.size(2)), int(v_1450.size(3))]
        v_1458 = F.unfold(input=v_1456, dilation=(1,1), kernel_size=(3,3), padding=(1,1), stride=(1,1))
        v_1459 = v_1454.view(*v_1455)
        v_1460 = F.softmax(input=v_1459, dim=2)
        v_1461 = v_1458.view(*v_1457)
        v_1462 = (v_1460 * v_1461)
        v_1463 = torch.sum(input=v_1462, dim=(2,), keepdim=False)
        v_1464 = [int(v_1450.size(0)), int(v_1450.size(1)), int((v_1450.size(2) * 4)), int((v_1450.size(3) * 4))]
        v_1465 = torch.permute(input=v_1463, dims=(0,1,4,2,5,3))
        v_1466 = v_1465.reshape(*v_1464)
        return v_1466

def export_torchscript():
    net = Model()
    net.eval()

    torch.manual_seed(0)
    v_0 = torch.rand(dtype=null)
    v_1 = torch.rand(dtype=null)

    mod = torch.jit.trace(net, (v_0, v_1))
    mod.save("flownet_544_pnnx.py.pt")

def export_onnx():
    net = Model()
    net.eval()

    torch.manual_seed(0)
    v_0 = torch.rand(dtype=null)
    v_1 = torch.rand(dtype=null)

    torch.onnx._export(net, (v_0, v_1), "flownet_544_pnnx.py.onnx", export_params=True, operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK, opset_version=13, input_names=['in0', 'in1'], output_names=['out0'])

def test_inference():
    net = Model()
    net.eval()

    torch.manual_seed(0)
    v_0 = torch.rand(dtype=null)
    v_1 = torch.rand(dtype=null)

    return net(v_0, v_1)
